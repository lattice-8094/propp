{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"PROPP: Pattern Recognition and Ontologies for Prose Processing","text":"","path":[""],"tags":[]},{"location":"#a-python-library-for-narrative-analysis","level":1,"title":"A Python library for narrative analysis","text":"<p>Propp is a modular NLP pipeline designed to extract rich character-centric information from narrative texts, especially litterature. It combines deep learning and rule-based techniques for:</p>","path":[""],"tags":[]},{"location":"#narratology-context","level":2,"title":"Narratology Context","text":"","path":[""],"tags":[]},{"location":"#computational-humanities","level":2,"title":"Computational Humanities","text":"","path":[""],"tags":[]},{"location":"#1-token-embeddings","level":3,"title":"1. Token Embeddings","text":"<p>Contextualized embeddings from pretrained transformer-based models (e.g., <code>google/mt5-xl</code>, <code>almanach/camembert-large</code>).</p>","path":[""],"tags":[]},{"location":"#2-mention-spans-detection","level":3,"title":"2. Mention-Spans Detection","text":"<p>Identifying spans in the text that refer to named or nominal entities, as well as referential pronouns.</p>","path":[""],"tags":[]},{"location":"#3-mention-type-prediction","level":3,"title":"3. Mention Type Prediction","text":"<p>Classifying detected mentions as persons, locations, organizations, geopolitical entities.</p>","path":[""],"tags":[]},{"location":"#4-coreference-resolution","level":3,"title":"4. Coreference Resolution","text":"<p>Linking mentions that refer to the same underlying entity across the narrative.</p>","path":[""],"tags":[]},{"location":"#5-characters-detection","level":3,"title":"5. Characters Detection","text":"<p>Consolidating coreferent mentions into coherent character entities.</p>","path":[""],"tags":[]},{"location":"#6-character-related-attributes-extraction","level":3,"title":"6. Character-Related Attributes Extraction","text":"<p>Inferring gender, social role, narrative function, and other descriptors from context.</p>","path":[""],"tags":[]},{"location":"#7-character-representation","level":3,"title":"7. Character Representation","text":"<p>Producing structured profiles for characters (e.g., via embeddings), ready for downstream analysis or visualization.</p>","path":[""],"tags":[]},{"location":"#installation","level":2,"title":"Installation","text":"<pre><code>pip install propp_fr\n</code></pre>","path":[""],"tags":[]},{"location":"FAQ/","level":1,"title":"Frequent Errors","text":"<pre><code>spacy_model, mentions_detection_model, coreference_resolution_model = load_models()\n\n----&gt; ValueError: Cannot use GPU, CuPy is not installed\n</code></pre> <p>For spacy to run on the GPU you need to install CuPy compatible with your installed CUDA version.</p> <p>To know CUDA version you can get</p> <pre><code>nvidia-smi\n</code></pre> <p>CUDA Version: 12.5</p>","path":["Frequent Errors"],"tags":[]},{"location":"_markdown/","level":1,"title":"Markdown in 5min","text":"","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#headers","level":2,"title":"Headers","text":"<pre><code># H1 Header\n## H2 Header\n### H3 Header\n#### H4 Header\n##### H5 Header\n###### H6 Header\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#text-formatting","level":2,"title":"Text formatting","text":"<pre><code>**bold text**\n*italic text*\n***bold and italic***\n~~strikethrough~~\n`inline code`\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#links-and-images","level":2,"title":"Links and images","text":"<pre><code>[Link text](https://example.com)\n[Link with title](https://example.com \"Hover title\")\n![Alt text](image.jpg)\n![Image with title](image.jpg \"Image title\")\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#lists","level":2,"title":"Lists","text":"<pre><code>Unordered:\n- Item 1\n- Item 2\n  - Nested item\n\nOrdered:\n1. First item\n2. Second item\n3. Third item\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#blockquotes","level":2,"title":"Blockquotes","text":"<pre><code>&gt; This is a blockquote\n&gt; Multiple lines\n&gt;&gt; Nested quote\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#code-blocks","level":2,"title":"Code blocks","text":"<pre><code>```javascript\nfunction hello() {\n  console.log(\"Hello, world!\");\n}\n```\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#tables","level":2,"title":"Tables","text":"<pre><code>| Header 1 | Header 2 | Header 3 |\n|----------|----------|----------|\n| Row 1    | Data     | Data     |\n| Row 2    | Data     | Data     |\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#horizontal-rule","level":2,"title":"Horizontal rule","text":"<pre><code>---\nor\n***\nor\n___\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#task-lists","level":2,"title":"Task lists","text":"<pre><code>- [x] Completed task\n- [ ] Incomplete task\n- [ ] Another task\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#escaping-characters","level":2,"title":"Escaping characters","text":"<pre><code>Use backslash to escape: \\* \\_ \\# \\`\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#line-breaks","level":2,"title":"Line breaks","text":"<pre><code>End a line with two spaces  \nto create a line break.\n\nOr use a blank line for a new paragraph.\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"algorithms/","level":1,"title":"Algorithms","text":"","path":["Algorithms"],"tags":[]},{"location":"algorithms/#mention-spans-detection-model","level":2,"title":"Mention Spans Detection Model","text":"","path":["Algorithms"],"tags":[]},{"location":"algorithms/#coreference-resolution-model","level":2,"title":"Coreference Resolution Model","text":"<p>Blog article about Coreference Resolution - Soon Taking images from Cergy JE + MATE-SHS</p> <p></p> <p></p>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#research-articles","level":3,"title":"Research Articles","text":"<p>Antoine Bourgois and Thierry Poibeau. 2025. The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works. In Proceedings of the Eighth Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2025). EMNLP 2025, Suzhou, China. arXiv, HAL, Poster</p>","path":["Algorithms"],"tags":[]},{"location":"contributors/","level":1,"title":"Contributors to the Propp Project","text":"","path":["Contributors"],"tags":[]},{"location":"contributors/#antoine-bourgois","level":2,"title":"Antoine Bourgois","text":"<ul> <li>Main Developer, Python Library, French Dataset Annotations, Model Training and Evaluation</li> </ul>","path":["Contributors"],"tags":[]},{"location":"contributors/#thierry-poibeau","level":2,"title":"Thierry Poibeau","text":"<ul> <li>Project Supervisor</li> </ul>","path":["Contributors"],"tags":[]},{"location":"datasets/","level":1,"title":"Datasets","text":"","path":["Datasets"],"tags":[]},{"location":"datasets/#french-dataset","level":2,"title":"French Dataset","text":"","path":["Datasets"],"tags":[]},{"location":"datasets/#litbank-fr","level":3,"title":"Litbank-fr","text":"","path":["Datasets"],"tags":[]},{"location":"processing_all_directory/","level":1,"title":"Processing all .txt Files in a Directory","text":"You can copy / paste the whole Notebook Code <pre><code>from propp_fr import load_models, load_text_file, generate_tokens_df, load_tokenizer_and_embedding_model, get_embedding_tensor_from_tokens_df, generate_entities_df, add_features_to_entities, perform_coreference, extract_attributes, generate_characters_dict, save_tokens_df, save_entities_df, save_book_file\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfiles_directory = \"&lt;directory_containing_txt_files&gt;\"\n\ntxt_files = sorted(p.stem for p in Path(files_directory).iterdir() if p.suffix == \".txt\")\nbook_files = sorted(p.stem for p in Path(files_directory).iterdir() if p.suffix == \".book\")\n\nunprocessed_files = [file for file in txt_files if file not in book_files]\nprint(f\"Unprocessed Files: {len(unprocessed_files):,}\")\n\nspacy_model, mentions_detection_model, coreference_resolution_model = load_models()\ntokenizer, embedding_model = load_tokenizer_and_embedding_model(mentions_detection_model[\"base_model_name\"])\n\nfor file_name in tqdm(unprocessed_files, desc=\"Processing .txt Files\"):\n    print(f\"Processing: {file_name}...\")\n    text_content = load_text_file(file_name, files_directory)\n    tokens_df = generate_tokens_df(text_content, spacy_model)\n    tokens_embedding_tensor = get_embedding_tensor_from_tokens_df(\n        text_content,\n        tokens_df,\n        tokenizer,\n        embedding_model\n    )\n\n    entities_df = generate_entities_df(\n        tokens_df,\n        tokens_embedding_tensor,\n        mentions_detection_model,\n    )\n\n    entities_df = add_features_to_entities(entities_df, tokens_df)\n\n    entities_df = perform_coreference(\n        entities_df,\n        tokens_embedding_tensor,\n        coreference_resolution_model,\n        propagate_coref=True,\n        rule_based_postprocess=False,\n    )\n\n    tokens_df = extract_attributes(entities_df, tokens_df)\n\n    characters_dict = generate_characters_dict(tokens_df, entities_df)\n\n\n    save_tokens_df(tokens_df, file_name, files_directory)\n    save_entities_df(entities_df, file_name, files_directory)\n    save_book_file(characters_dict, file_name, files_directory)\n</code></pre>","path":["User Guide","Basic Processing","Processing Full Directory"],"tags":[]},{"location":"quick_start/","level":1,"title":"Quick Start","text":"<p>Getting Started with French Text Processing</p> <p>This guide will help you load and analyze French text files using the <code>propp_fr</code> library.</p>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#installation","level":2,"title":"Installation","text":"<p>The French variant of the Propp python library can be installed via pypi:</p> <pre><code>pip install propp_fr\n</code></pre>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#oneliner-processing","level":2,"title":"Oneliner Processing","text":"<p>You can process a text file in one line with the default models:</p> <pre><code>from propp_fr import process_text_file\n\nprocess_text_file(\"root_directory/my_french_novel.txt\")\n</code></pre> <p>This will generate three additional files in the same directory:</p> <pre><code>root_directory/\n‚îú‚îÄ‚îÄ my_french_novel.txt\n‚îú‚îÄ‚îÄ my_french_novel.tokens\n‚îú‚îÄ‚îÄ my_french_novel.entities\n‚îî‚îÄ‚îÄ my_french_novel.book\n</code></pre> <ul> <li> <p><code>my_french_novel.tokens</code> contains all tokens along with:</p> </li> <li> <p>Part-of-speech tags  </p> </li> <li> <p>Syntactic parsing information  </p> </li> <li> <p><code>my_french_novel.entities</code> contains information about recognized entities, including:</p> </li> <li> <p>Start and end positions  </p> </li> <li> <p>Entity type  </p> </li> <li> <p><code>my_french_novel.book</code> contains all characters and their attributes, including:</p> </li> <li> <p>Coreference information  </p> </li> <li>Gender, number, and other features  </li> </ul>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#reloading-processed-files","level":2,"title":"Reloading Processed Files","text":"<p>Generated files are loaded by:</p> <pre><code>from propp_fr import load_text_file, load_tokens_df, load_entities_df, load_book_file\n\nfile_name = \"my_french_novel\"\nroot_directory = \"root_directory\"\n\ntext_content = load_text_file(file_name, root_directory)\ntokens_df = load_tokens_df(file_name, root_directory)\nentities_df = load_entities_df(file_name, root_directory)\ncharacters_dict = load_book_file(file_name, root_directory)\n</code></pre>","path":["User Guide","Quick Start"],"tags":[]},{"location":"step_by_step_processing/","level":1,"title":"Step by Step Processing","text":"","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#1-loading-models","level":2,"title":"1: Loading Models","text":"<pre><code>from propp_fr import load_models\nspacy_model, mentions_detection_model, coreference_resolution_model = load_models()\n</code></pre> <p>Default models are:</p> <ul> <li><code>spacy_model</code>: fr_dep_news_trf</li> <li><code>mentions_detection_model</code>: propp-fr_NER_camembert-large_FAC_GPE_LOC_PER_TIME_VEH</li> <li><code>coreference_resolution_model</code>: propp-fr_coreference-resolution_camembert-large_PER</li> </ul>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#2-loading-a-txt-file","level":2,"title":"2: Loading a .txt File","text":"<pre><code>from propp_fr import load_text_file\ntext_content = load_text_file(\"root_directory/my_french_novel.txt\")\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#3-tokenizing-the-text","level":2,"title":"3: Tokenizing the Text","text":"<p>Break down the text into individual tokens (words and punctuation) with linguistic information:</p> <pre><code>from propp_fr import generate_tokens_df\ntokens_df = generate_tokens_df(text_content, spacy_model)\n</code></pre> <p><code>tokens_df</code> is a <code>pandas.DataFrame</code> where each row represents one token from the text.</p> Column Name Description <code>paragraph_ID</code> Which paragraph the token belongs to <code>sentence_ID</code> Which sentence the token belongs to <code>token_ID_within_sentence</code> Position of the token within its sentence <code>token_ID_within_document</code> Position of the token in the entire document <code>word</code> The actual word as it appears in the text <code>lemma</code> The base/dictionary form of the word <code>byte_onset</code> Starting byte position in the original file <code>byte_offset</code> Ending byte position in the original file <code>POS_tag</code> Part-of-speech tag (noun, verb, adjective, etc.) <code>dependency_relation</code> How the word relates to other words <code>syntactic_head_ID</code> The ID of the word this token depends on","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#4-embedding-tokens","level":2,"title":"4: Embedding Tokens","text":"<p>Transform the tokens into numerical representations (embeddings) that capture their meaning:</p> <pre><code>from propp_fr import load_tokenizer_and_embedding_model, get_embedding_tensor_from_tokens_df\n\n# Load the tokenizer and pre-trained embedding model\ntokenizer, embedding_model = load_tokenizer_and_embedding_model(\n    mentions_detection_model[\"base_model_name\"],\n  )\n\n# Generate embeddings for all tokens\ntokens_embedding_tensor = get_embedding_tensor_from_tokens_df(\n    text_content,\n    tokens_df,\n    tokenizer,\n    embedding_model,\n  )\n</code></pre> <p><code>tokens_embedding_tensor</code> is a <code>torch.tensor</code> object with dimensions <code>[number_of_tokens, embedding_size]</code>.</p> <p>Each row corresponds to one token from <code>tokens_df</code>, preserving the same order.</p> <p>These embeddings will be used as inputs for the <code>mention detection model</code> and the <code>coreference resolution model</code>.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#5-mention-spans-detection","level":2,"title":"5: Mention Spans Detection","text":"<p>Identify all mentions belonging to entities of different types in the text:</p> <ul> <li>Characters (PER): pronouns (je, tu, il, ...), possessive pronouns (mon, ton, son, ...), common nouns (le capitaine, la princesse, ...) and proper nouns (Indiana Delmare, Honor√© de Pardaillan, ...)  </li> <li>Facilities (FAC): chat√™au, sentier, chambre, couloir, ...  </li> <li>Time (TIME): le r√®gne de Louis XIV, ce matin, en juillet, ...  </li> <li>Geo-Political Entities (GPE): Montrouge, France, le petit hameau, ...  </li> <li>Locations (LOC): le sud, Mars, l'oc√©an, le bois, ...  </li> <li>Vehicles (VEH): avion, voitures, cal√®che, v√©los, ...</li> </ul> <pre><code>from propp_fr import generate_entities_df\n\nentities_df = generate_entities_df(\n    tokens_df,\n    tokens_embedding_tensor,\n    mentions_detection_model,\n)\n</code></pre> <p>What this does: Scans through the text to find all mentions of entities.</p> <p>The <code>entities_df</code> object is a <code>pandas.DataFrame</code> where each row represents a detected mention:</p> Column Name Description <code>start_token</code> Token ID where the mention begins <code>end_token</code> Token ID where the mention ends <code>cat</code> Type of the entity <code>confidence</code> Model's confidence score (0-1) for this detection <code>text</code> The actual text of the mention <p>To learn more about how mention detection is performed under the hood, check the Algorithms Section</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#6-adding-linguistic-features","level":2,"title":"6: Adding Linguistic Features","text":"<p>Enrich your entity mentions with additional grammatical and syntactic information:</p> <pre><code>from propp_fr import add_features_to_entities\n\nentities_df = add_features_to_entities(entities_df, tokens_df)\n</code></pre> <p>What this does: Adds detailed linguistic features to each mention, including grammatical properties, syntactic structure, and contextual information.</p> <p>This step adds the following columns to <code>entities_df</code>:</p> Column Name Description <code>mention_len</code> Length of the mention in tokens <code>paragraph_ID</code> Paragraph containing the mention <code>sentence_ID</code> Sentence containing the mention <code>start_token_ID_within_sentence</code> Position where mention starts in its sentence <code>out_to_in_nested_level</code> Nesting depth (outer to inner) <code>in_to_out_nested_level</code> Nesting depth (inner to outer) <code>nested_entities_count</code> Number of entities nested within this mention <code>head_id</code> ID of the syntactic head token <code>head_word</code> The actual head word <code>head_dependency_relation</code> Dependency relation of the head <code>head_syntactic_head_ID</code> ID of the head's syntactic parent <code>POS_tag</code> Part-of-speech tag of the head <code>prop</code> Mention type: pronoun (PRON), common noun (NOM), or proper noun (PROP) <code>number</code> Grammatical number (singular/plural) <code>gender</code> Grammatical gender (masculine/feminine) <code>grammatical_person</code> Grammatical person (1st, 2nd, 3rd) <p>These features are primarily used in the following steps of <code>coreference resolution</code> and <code>character representation</code>, but they can also be leveraged directly for a range of literary and linguistic analyses, such as character centrality, proper name tracking, mention type distribution, gender representation, and narrative perspective.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#7-coreference-resolution","level":2,"title":"7: Coreference Resolution","text":"<p>Link all <code>PER</code> mentions that refer to the same character, creating coreference chains:</p> <pre><code>from propp_fr import perform_coreference\n\nentities_df = perform_coreference(\n    entities_df,\n    tokens_embedding_tensor,\n    coreference_resolution_model,\n    )\n</code></pre> <p>What this does: Groups character mentions into coreference chains where all mentions in a chain refer to the same person. For example, <code>Marie</code>, <code>she</code>, and <code>the young woman</code> might all be linked together as referring to the same character.</p> <p>This step adds one new column to <code>entities_df</code>:</p> Column Name Description <code>COREF</code> ID of the coreference chain this mention belongs to <p>Mentions with the same <code>COREF</code> value refer to the same character. Mentions with different values refer to different characters.</p> <p>To learn more about how coreference resolution is performed under the hood, check the Algorithms Section</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#8-extracting-character-attributes","level":2,"title":"8: Extracting Character Attributes","text":"<p>Identify tokens that describe or relate to characters:</p> <pre><code>from propp_fr import extract_attributes\ntokens_df = extract_attributes(entities_df, tokens_df)\n</code></pre> <p>What this does: Analyzes the syntactic structure around character mentions to identify words that function as attributes, linking them to the characters they describe.</p> <p>This step adds the following columns to <code>tokens_df</code>:</p> Column Name Description <code>is_mention_head</code> Whether this token is the head of a character mention <code>char_att_agent</code> Mention ID if token is an agent attribute, -1 otherwise <code>char_att_patient</code> Mention ID if token is a patient attribute, -1 otherwise <code>char_att_mod</code> Mention ID if token is a modifier attribute, -1 otherwise <code>char_att_poss</code> Mention ID if token is a possessive attribute, -1 otherwise <p>For each token, if it serves as an attribute to a character, the corresponding column contains the syntactic head token ID of the mention. Otherwise, it contains -1.</p> <p>The four attribute types:</p> <ul> <li><code>Agent</code>: verbs where the character is the subject (actions they perform): Marie marche, elle parle</li> <li><code>Patient</code>: verbs where the character is the direct object or passive subject (actions done to them): on pousse Jean, il est suivi</li> <li><code>Modifier</code>: adjectives or nominal predicates describing the character: Hercule est fort, la grande reine, Victor Hugo, l' √©crivain</li> <li><code>Possessive</code>: nouns denoting possessions linked by determiners, de-genitives, or avoir: son √©p√©e, la maison de Alis√©e, il a un chien</li> </ul>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#9-aggregating-character-information","level":2,"title":"9: Aggregating Character Information","text":"<p>Build a unified, structured representation of every character extracted so far:</p> <pre><code>from propp_fr import generate_characters_dict\n\ncharacters_dict = generate_characters_dict(tokens_df, entities_df)\n</code></pre> <p>This function gathers all relevant information from every mention of each character: surface forms, syntactic attributes, and inferred features such as gender or number.</p> <p><code>characters_dict</code> is a list of dictionaries, where each dictionary corresponds to a single character and contains the following fields:</p> Key Description <code>id</code> Character identifier (0 = most frequent) <code>count</code> Total mentions and proportion relative to all character mentions <code>gender</code> Ratio of gendered mentions and inferred overall gender (based on majority evidence) <code>number</code> Ratio of singular/plural mentions and inferred number <code>mentions</code> All surface forms used to refer to the character: proper nouns, common nouns, and pronouns <code>agent</code> List of all agent attributes linked to the character <code>patient</code> list of all patient attributes linked to the character <code>mod</code> list of all modifiers attributes linked to the character <code>poss</code> list of all possessives attributes linked to the character <p>The resulting <code>characters_dict</code> provides a complete, query-ready profile for each character.  This will be useful for narrative analysis, visualization, and computational literary studies.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#saving-generated-output","level":2,"title":"Saving Generated Output","text":"<p>Once processing is complete, you can save the generated files for future use, storing your processed tokens, entities, and character profiles as reusable data so you don‚Äôt need to re-run the entire pipeline.</p> <pre><code>from propp_fr import save_tokens_df, save_entities_df, save_book_file\n\nroot_directory = \"root_directory\"\nfile_name = \"my_french_novel\"\n\nsave_tokens_df(tokens_df, file_name, root_directory)\nsave_entities_df(entities_df, file_name, root_directory)\nsave_book_file(characters_dict, file_name, root_directory)\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#reloading-processed-files","level":2,"title":"Reloading Processed Files","text":"<p>Later, you can reload the generated files instantly to resume your analysis.</p> <pre><code>from propp_fr import load_text_file, load_tokens_df, load_entities_df, load_book_file\n\nfile_name = \"my_french_novel\"\nroot_directory = \"root_directory\"\n\ntext_content = load_text_file(file_name, root_directory)\ntokens_df = load_tokens_df(file_name, root_directory)\nentities_df = load_entities_df(file_name, root_directory)\ncharacters_dict = load_book_file(file_name, root_directory)\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"training/","level":1,"title":"Training a new Pipeline","text":"<p>Training a new coreference resolution pipeline from scratch</p>","path":["Training a new Pipeline"],"tags":[]},{"location":"training/#dataset-preparation","level":2,"title":"Dataset Preparation","text":"<p>First, you need an annotated dataset.</p> <p>This dataset should contain:</p> <ul> <li>The raw text files</li> <li>The annotations minimal infos:<ul> <li>The start and end of the annotation (character indexes in the raw text)</li> <li>The label of the annotation (type of entity)</li> <li>The coreference chains ID  </li> </ul> </li> </ul> <p>We will make an example by training a pipeline on the LitBank Dataset</p> <p>We first need to download the dataset from the github repository: https://github.com/dbamman/litbank/tree/master/coref/brat</p> Python Code to Download the Dataset <pre><code>import os\n\nlocal_dataset_path = \"datasets/litbank\"\nos.makedirs(dataset_path, exist_ok=True)\n\n# The specific GitHub folder you want\ngithub_folder_path = \"coref/brat\"\nrepo_url = \"https://github.com/dbamman/litbank.git\"\n\n# Run the git commands (note the exclamation mark !)\n!cd {dataset_path} &amp;&amp; git init\n!cd {dataset_path} &amp;&amp; git remote add origin -f {repo_url}\n!cd {dataset_path} &amp;&amp; git config core.sparseCheckout true\n!cd {dataset_path} &amp;&amp; echo \"{github_folder_path}/*\" &gt;&gt; .git/info/sparse-checkout\n!cd {dataset_path} &amp;&amp; git pull origin master\n# Reorganize: move the brat directory to the root of dataset_path, then remove coref and .git\n!cd {dataset_path} &amp;&amp; mv coref/brat . &amp;&amp; rm -rf coref &amp;&amp; rm -rf .git\n</code></pre> <p>We can now take a look at the dataset:</p> Python Code <pre><code>from pathlib import Path\nfrom collections import Counter\n\nraw_files_path = \"datasets/litbank/tsv\"\n\nall_files = [p for p in Path(raw_files_path).iterdir() if p.is_file()]\n\nextensions = dict(Counter(p.suffix for p in files).most_common())\nprint(f\"The dataset contains {len(all_files):,} files with the following extensions:\")\nfor extension, count in extensions.items():\n    print(f\"\\t'{extension}' = {count:,}\")\n</code></pre> <pre><code>The dataset contains 200 files with the following extensions:\n    '.txt' = 100\n    '.ann' = 100\n</code></pre> <p>While the <code>.txt</code> file contains the raw text, we need to take a closer look at the <code>.ann</code> file.</p> Python Code <pre><code>txt_files = [p for p in all_files if p.suffix == \".txt\"]\nwith open(ann_files[0], \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n\nann_files = [p for p in all_files if p.suffix == \".ann\"]\nann_df = pd.read_csv(ann_files[0], sep=\"\\t\", header=None)\n\nprint(ann_df.sample(5).to_markdown())\n</code></pre> 0 1 2 3 4 5 6 7 8 393 COREF T174 Ralph_Percy-0 nan nan nan nan nan nan 18 MENTION T27 15 45 15 46 my house FAC NOM 42 MENTION T65 70 4 70 4 wife PER NOM 335 COREF T135 settlers-7 nan nan nan nan nan nan 245 MENTION T254 51 0 51 0 I PER PRON <p>We see that the first column contains the annotation type, on which the content of other columns depends.</p> Python Code <pre><code>type_to_description_mapping = {\n    \"MENTION\": \"Entity mention with span and semantic type\",\n    \"COREF\": \"True coreference link between two mentions\",\n    \"COP\": \"Copular relation between two mentions (not treated as coreference)\",\n    \"APPOS\": \"Appositional relation between two mentions (not treated as coreference)\",\n}\n\nrow_types_df = pd.DataFrame(\n        Counter(ann_df[0]).most_common(),\n        columns=[\"Column '0' Type\", \"Count\"]\n    )\nrow_types_df[\"Row Type\"] = row_types_df[\"Column '0' Type\"].map(type_to_description_mapping)\n\nprint(row_types_df.to_markdown(index=False))\n</code></pre> Column '0' Type Count Row Type MENTION 316 Entity mention COREF 307 True coreference link between two mentions COP 6 Copular relation between two mentions (not treated as coreference) APPOS 3 Appositional relation between two mentions (not treated as coreference) <p>In this case, only the <code>MENTION</code> and <code>COREF</code> annotations are relevant. </p> <p>We will create two new objects for both the mentions and the coreference links:</p>","path":["Training a new Pipeline"],"tags":[]},{"location":"training/#russian","level":2,"title":"Russian","text":"<p>üöß Coming soon... üöß</p>","path":["Training a new Pipeline"],"tags":[]},{"location":"use_cases/","level":1,"title":"Use Cases","text":"","path":["Use Cases"],"tags":[]},{"location":"use_cases/#detective-archetype","level":2,"title":"Detective Archetype","text":"<p>Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature</p> <p>This research explores the evolution of the detective archetype in French detective fiction through computational analysis. Using quantitative methods and character-level embeddings, we show that a supervised model is able to capture the unity of the detective archetype across 150 years of literature, from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding, the study demonstrates how the detective figure evolves from a secondary narrative role to become the central character and the ‚Äúreasoning machine‚Äù of the classical detective story. In the aftermath of the Second World War, with the importation of the hardboiled tradition into France, the archetype becomes more complex, navigating the genre‚Äôs turn toward social violence and moral ambiguity.</p> <p> </p> <p>Cite this work <pre><code>@article{10.63744@SMbYIWcHZj87,\n  title = {Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature},\n  author = {Jean Barr√© and Olga Seminck and Antoine Bourgois and Thierry Poibeau},\n  year = {2025},\n  journal = {Anthology of Computers and the Humanities},\n  volume = {3},\n  pages = {983--999},\n  editor = {Taylor Arnold, Margherita Fantoli, and Ruben Ros},\n  doi = {10.63744/SMbYIWcHZj87}\n}\n</code></pre></p>","path":["Use Cases"],"tags":[]},{"location":"use_cases/#character-representation-ontology","level":2,"title":"Character Representation - Ontology","text":"","path":["Use Cases"],"tags":[]}]}