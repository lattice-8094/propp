{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Propp ‚Äì NLP Framework for Narrative Analysis","text":"PROPP <p>   Pattern Recognition and Ontologies for Prose Processing   </p> <p>   A Research-Driven Python NLP Framework for Narrative Analysis   </p> <p> Propp is a Python-based Natural Language Processing (NLP) framework developed for the analysis of narrative texts, with a focus on long-form literary fiction.   </p> <p>     Designed for computational literary studies, Propp bridges Natural Language Processing and narrative theory to enable character-centric, ontology-based, and corpus-scale analysis of narratives.   </p> <p>     The framework is intended for research on characters, narrative structure, and large-scale literary corpora, while remaining accessible to scholars with limited programming experience.   </p>      Get Started         Learn More","path":["Propp ‚Äì NLP Framework for Narrative Analysis"],"tags":[]},{"location":"API/","level":1,"title":"API","text":"","path":["API"],"tags":[]},{"location":"API/#propp_frload_text_file","level":2,"title":"propp_fr.load_text_file","text":"<pre><code>propp_fr.load_text_file(file_name: str, files_directory: str = \"\", extension: str = \".txt\") ‚Üí str\n</code></pre> <p>Loads and returns the content of a text file from a specified directory.</p> <p>This function automatically appends the file extension if not already present in the filename, constructs the full file path using <code>pathlib.Path</code>, and reads the entire content of the file using UTF-8 encoding.</p> propp_fr.load_text_file <p>Parameters</p> <ul> <li>file_name (str) ‚Äì The name of the file to load. If the extension is not included, it will be automatically appended.</li> <li>files_directory (str, optional) ‚Äì The directory path where the file is located. If empty, looks in the current working directory. Default: <code>\"\"</code>.</li> <li>extension (str, optional) ‚Äì The file extension to append if not present in <code>file_name</code>. Default: <code>\".txt\"</code>.</li> </ul>","path":["API"],"tags":[]},{"location":"API/#returns","level":3,"title":"Returns","text":"<p>str ‚Äì The complete text content of the file as a string.</p>","path":["API"],"tags":[]},{"location":"API/#raises","level":3,"title":"Raises","text":"<ul> <li>FileNotFoundError ‚Äì If the specified file does not exist at the given path.</li> <li>PermissionError ‚Äì If the file cannot be read due to insufficient permissions.</li> <li>UnicodeDecodeError ‚Äì If the file cannot be decoded using UTF-8 encoding.</li> </ul>","path":["API"],"tags":[]},{"location":"API/#examples","level":3,"title":"Examples","text":"<pre><code># Load a file from the current directory\ncontent = propp_fr.load_text_file(\"example\")\n# Loads \"example.txt\" from current directory\n\n# Load a file with explicit extension\ncontent = propp_fr.load_text_file(\"data.txt\")\n# Loads \"data.txt\" from current directory\n\n# Load from a specific directory\ncontent = propp_fr.load_text_file(\"story\", files_directory=\"./texts\")\n# Loads \"./texts/story.txt\"\n\n# Load a file with custom extension\ncontent = propp_fr.load_text_file(\"config\", extension=\".cfg\")\n# Loads \"config.cfg\" from current directory\n\n# Full path example\ncontent = propp_fr.load_text_file(\"chapter1\", files_directory=\"/data/books\", extension=\".md\")\n# Loads \"/data/books/chapter1.md\"\n</code></pre>","path":["API"],"tags":[]},{"location":"API/#notes","level":3,"title":"Notes","text":"<ul> <li>The function uses <code>pathlib.Path</code> for cross-platform path handling.</li> <li>The function always reads files using UTF-8 encoding.</li> <li>Extension is automatically added only if the filename doesn't already end with it.</li> <li>The entire file content is loaded into memory at once, which may not be suitable for very large files.</li> </ul>","path":["API"],"tags":[]},{"location":"FAQ/","level":1,"title":"Frequent Errors","text":"<pre><code>spacy_model, mentions_detection_model, coreference_resolution_model = load_models()\n\n----&gt; ValueError: Cannot use GPU, CuPy is not installed\n</code></pre> <p>For spacy to run on the GPU you need to install CuPy compatible with your installed CUDA version.</p> <p>To know CUDA version you can get</p> <pre><code>nvidia-smi\n</code></pre> <p>CUDA Version: 12.5</p>","path":["Frequent Errors"],"tags":[]},{"location":"_markdown/","level":1,"title":"Markdown in 5min","text":"","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#headers","level":2,"title":"Headers","text":"<pre><code># H1 Header\n## H2 Header\n### H3 Header\n#### H4 Header\n##### H5 Header\n###### H6 Header\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#text-formatting","level":2,"title":"Text formatting","text":"<pre><code>**bold text**\n*italic text*\n***bold and italic***\n~~strikethrough~~\n`inline code`\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#links-and-images","level":2,"title":"Links and images","text":"<pre><code>[Link text](https://example.com)\n[Link with title](https://example.com \"Hover title\")\n![Alt text](image.jpg)\n![Image with title](image.jpg \"Image title\")\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#lists","level":2,"title":"Lists","text":"<pre><code>Unordered:\n- Item 1\n- Item 2\n  - Nested item\n\nOrdered:\n1. First item\n2. Second item\n3. Third item\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#blockquotes","level":2,"title":"Blockquotes","text":"<pre><code>&gt; This is a blockquote\n&gt; Multiple lines\n&gt;&gt; Nested quote\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#code-blocks","level":2,"title":"Code blocks","text":"<pre><code>```javascript\nfunction hello() {\n  console.log(\"Hello, world!\");\n}\n```\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#tables","level":2,"title":"Tables","text":"<pre><code>| Header 1 | Header 2 | Header 3 |\n|----------|----------|----------|\n| Row 1    | Data     | Data     |\n| Row 2    | Data     | Data     |\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#horizontal-rule","level":2,"title":"Horizontal rule","text":"<pre><code>---\nor\n***\nor\n___\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#task-lists","level":2,"title":"Task lists","text":"<pre><code>- [x] Completed task\n- [ ] Incomplete task\n- [ ] Another task\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#escaping-characters","level":2,"title":"Escaping characters","text":"<pre><code>Use backslash to escape: \\* \\_ \\# \\`\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"_markdown/#line-breaks","level":2,"title":"Line breaks","text":"<pre><code>End a line with two spaces  \nto create a line break.\n\nOr use a blank line for a new paragraph.\n</code></pre>","path":["Markdown in 5min"],"tags":[]},{"location":"about/","level":1,"title":"About","text":"<p>Propp is a Python-based Natural Language Processing (NLP) framework developed for the analysis of narrative texts, with a focus on long-form literary fiction. </p> <p>Designed for computational literary studies, Propp bridges Natural Language Processing and narrative theory to enable character-centric, ontology-based, and corpus-scale analysis of prose narratives.</p> <p>The framework is designed for research on characters, narrative structure, and large-scale literary corpora, while remaining accessible to scholars with limited programming experience.</p>","path":["About"],"tags":[]},{"location":"about/#objectives","level":2,"title":"Objectives","text":"<p>Bridging narratological theory and contemporary research in natural language processing.</p>","path":["About"],"tags":[]},{"location":"about/#narratology-context","level":2,"title":"Narratology Context","text":"<p>Vladimir Propp</p>","path":["About"],"tags":[]},{"location":"about/#computational-humanities","level":2,"title":"Computational Humanities","text":"","path":["About"],"tags":[]},{"location":"about/#design-philosophy","level":2,"title":"Design Philosophy","text":"<ul> <li>Modularity</li> <li>Open Source</li> <li>Informed by narratology theory</li> </ul>","path":["About"],"tags":[]},{"location":"about/#french-fiction-and-beyond","level":2,"title":"French Fiction and Beyond","text":"<p>While the starting point of the project is long form French fiction narratives (novels) we aim at exploring other types of nnarratives.</p> <p>Dream narratives, altered state of consciousness narratives, personal narratives, myths and legends.</p>","path":["About"],"tags":[]},{"location":"about/#project-lineage","level":2,"title":"Project Lineage","text":"<ul> <li> <p>2014 - Java BookNLP</p> <ul> <li>A Bayesian Mixed Effects Model of Literary Character (Bamman et al., ACL 2014)</li> </ul> </li> <li> <p>2019 - BookNLP</p> <ul> <li>An annotated dataset of literary entities (Bamman et al., NAACL 2019)</li> </ul> </li> <li> <p>2020 - Multilingual BookNLP</p> </li> <li> <p>2021 - BookNLP-fr</p> <ul> <li>BookNLP-fr, the French Versant of BookNLP. A Tailored Pipeline for 19th and 20th Century French Literature (M√©lanie et al., JCLS 2024)</li> </ul> </li> <li> <p>2025 - Propp</p> </li> </ul>","path":["About"],"tags":[]},{"location":"about/#modules","level":2,"title":"Modules","text":"<p>Propp framework combines deep learning and rule-based techniques for:</p> <ol> <li> <p>Token Embeddings</p> <p>Contextualized embeddings from pretrained transformer-based models (e.g., <code>google/mt5-xl</code>, <code>almanach/camembert-large</code>).</p> </li> <li> <p>Mention-Spans Detection</p> <p>Identifying spans in the text that refer to named or nominal entities, as well as referential pronouns.</p> </li> <li> <p>Mention Type Prediction</p> <p>Classifying detected mentions as persons, locations, organizations, geopolitical entities.</p> </li> <li> <p>Coreference Resolution</p> <p>Linking mentions that refer to the same underlying entity across the narrative.</p> </li> <li> <p>Characters Detection</p> <p>Consolidating coreferent mentions into coherent character entities.</p> </li> <li> <p>Character-Related Attributes Extraction </p> <p>Inferring gender, social role, narrative function, and other descriptors from context.</p> </li> <li> <p>Character Representation</p> <p>Producing structured profiles for characters (e.g., via embeddings), ready for downstream analysis or visualization.</p> </li> </ol>","path":["About"],"tags":[]},{"location":"about/#funding","level":2,"title":"Funding","text":"<p>This project was funded in part by the PRAIRIE‚ÄìPSAI (Paris Artificial Intelligence Research Institute ‚Äì Paris School of Artificial Intelligence), reference ANR-22-IACL-0008.</p> <p>The project also received support from the Major Research Program of PSL Research University, \"CultureLab\", launched by PSL Research University and implemented by the French National Research Agency (ANR), reference ANR-10-IDEX-0001.</p> <p>The project additionally benefited from the support of the Fondation de l‚ÄôENS, through the Be Ys Chair, dedicated to the support of innovative research projects in artificial intelligence.</p>","path":["About"],"tags":[]},{"location":"algorithms/","level":1,"title":"Algorithms","text":"","path":["Algorithms"],"tags":[]},{"location":"algorithms/#pipeline-overview","level":2,"title":"Pipeline Overview","text":"","path":["Algorithms"],"tags":[]},{"location":"algorithms/#mention-spans-detection-model","level":2,"title":"Mention Spans Detection Model","text":"<p>The mention detection module consists of two stacked BiLSTM-CRF models, each trained on a different nesting level of mentions. During inference, predicted spans from both models are combined. If two mention spans overlap, the span with the lower prediction confidence is discarded.</p>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#bert-embeddings","level":3,"title":"BERT Embeddings","text":"<p>The raw text is split into overlapping segments of length L (corresponding to the maximum context window of the embedding model), with an overlap of L/2 to maximize the contextual information available for each token. Each segment is processed by the CamemBERT-large model, and the last hidden layer is extracted as token representations (1024 dimensions).</p> <p>The final embedding for each token is computed as the average of its representations across overlapping segments. The CamemBERT model is not fine-tuned for this task.</p>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#bioes-tag-prediction","level":3,"title":"BIOES Tag Prediction","text":"<p>For each sentence, token representations are passed through the BiLSTM-CRF model, which outputs a sequence of BIOES tags:</p> <ul> <li>B-PER: Beginning of a mention</li> <li>I-PER: Inside a mention</li> <li>E-PER: End of a mention</li> <li>S-PER: Single-token mention</li> <li>O: Outside any mention</li> </ul>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#model-architecture","level":3,"title":"Model Architecture","text":"<ul> <li>Locked Dropout (0.5) applied to embeddings for regularization</li> <li>Projection Layer: Highway network mapping 1024 ‚Üí 2048 dimensions</li> <li>BiLSTM Layer: Single bidirectional LSTM with 256 hidden units per direction</li> <li>Linear Layer: Maps 512-dimensional BiLSTM outputs to BIOES label scores</li> <li>CRF Layer: Enforces structured consistency in label predictions</li> </ul>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#model-training","level":3,"title":"Model Training","text":"<ul> <li>Data Splitting: Leave-One-Out Cross-Validation (LOOCV) with an 85% / 15% train-validation split</li> <li>Batch Size: 16 sentences per batch</li> <li>Optimization: Adam optimizer</li> <li>Learning rate: 1.4 √ó 10‚Åª‚Å¥</li> <li>Weight decay: 1 √ó 10‚Åª‚Åµ</li> <li>Learning Rate Scheduling: ReduceLROnPlateau</li> <li>Factor: 0.5</li> <li>Patience: 2</li> <li>Average Training Epochs: 20</li> <li>Hardware: Trained on a single 6 GB NVIDIA RTX 1000 Ada Generation GPU</li> </ul>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#coreference-resolution-model","level":2,"title":"Coreference Resolution Model","text":"<p>Blog article about Coreference Resolution - Soon Taking images from Cergy JE + MATE-SHS</p> <p></p> <p></p>","path":["Algorithms"],"tags":[]},{"location":"algorithms/#research-articles","level":3,"title":"Research Articles","text":"Cite this workBibTeX <p>Antoine Bourgois and Thierry Poibeau. 2025. The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works. In Proceedings of the Eighth Workshop on Computational Models of Reference, Anaphora and Coreference,  pages 55‚Äì69,  Suzhou, China.  Association for Computational Linguistics.</p> <pre><code>@inproceedings{bourgois-poibeau-2025-elephant,\n    title = \"The Elephant in the Coreference Room: Resolving Coreference in Full-Length {F}rench Fiction Works\",\n    author = \"Bourgois, Antoine  and\n      Poibeau, Thierry\",\n    editor = \"Ogrodniczuk, Maciej  and\n      Novak, Michal  and\n      Poesio, Massimo  and\n      Pradhan, Sameer  and\n      Ng, Vincent\",\n    booktitle = \"Proceedings of the Eighth Workshop on Computational Models of Reference, Anaphora and Coreference\",\n    month = nov,\n    year = \"2025\",\n    address = \"Suzhou, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.crac-1.5/\",\n    doi = \"10.18653/v1/2025.crac-1.5\",\n    pages = \"55--69\",\n    abstract = \"While coreference resolution is attracting more interest than ever from computational literature researchers, representative datasets of fully annotated long documents remain surprisingly scarce. In this paper, we introduce a new annotated corpus of three full-length French novels, totaling over 285,000 tokens. Unlike previous datasets focused on shorter texts, our corpus addresses the challenges posed by long, complex literary works, enabling evaluation of coreference models in the context of long reference chains. We present a modular coreference resolution pipeline that allows for fine-grained error analysis. We show that our approach is competitive and scales effectively to long documents. Finally, we demonstrate its usefulness to infer the gender of fictional characters, showcasing its relevance for both literary analysis and downstream NLP tasks.\"\n}\n</code></pre>","path":["Algorithms"],"tags":[]},{"location":"available_datasets/","level":1,"title":"Available Datasets","text":"dataset_name language documents Tokens Entity Types Count COREF Average Tokens / Doc ontonotes5_english-NER en 3,637 2,074,405 18 ‚ùå 570 long-litbank-fr-PER-only fr 32 556,103 4 ‚úÖ 17,378 conll2003-NER en 1,393 319,965 4 ‚ùå 229 litbank-ru ru 145 291,290 7 ‚úÖ 2,008 litbank-fr fr 29 276,992 7 ‚úÖ 9,551 litbank en 100 213,677 6 ‚úÖ 2,136","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#ner-only-propp-formatted-datasets","level":2,"title":"NER-Only Propp formatted datasets","text":"<ul> <li>Download conll2003 NER dataset ‚Äì PROPP Minimal Implementation</li> </ul> <p>conll2003-NER Mention Spans Detection</p>","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#coreference-resolution-propp-formatted-datasets","level":2,"title":"Coreference Resolution Propp formatted datasets","text":"<ul> <li> <p>Download Long-LitBank-fr-PER-Only dataset ‚Äì PROPP Minimal Implementation</p> </li> <li> <p>Download LitBank-fr dataset ‚Äì PROPP Minimal Implementation</p> </li> <li> <p>Download LitBank dataset ‚Äì PROPP Minimal Implementation</p> <p>Note: This version is a minimal implementation of the original LitBank dataset, formatted specifically for use with Propp‚Äôs coreference resolution training pipeline. It contains only the essential columns (<code>byte_onset</code>, <code>byte_offset</code>, <code>cat</code>, <code>COREF_name</code>) aligned with the text for efficient model training.</p> </li> </ul>","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#french-datasets","level":2,"title":"French Datasets","text":"","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#litbank-fr","level":3,"title":"LitBank-fr","text":"","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#long-litbank-fr-characters-only","level":3,"title":"Long-LitBank-fr (characters only)","text":"Cite this workBibTeX <p>Antoine Bourgois and Thierry Poibeau. 2025. The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works. In Proceedings of the Eighth Workshop on Computational Models of Reference, Anaphora and Coreference,  pages 55‚Äì69,  Suzhou, China.  Association for Computational Linguistics.</p> <pre><code>@inproceedings{bourgois-poibeau-2025-elephant,\n    title = \"The Elephant in the Coreference Room: Resolving Coreference in Full-Length {F}rench Fiction Works\",\n    author = \"Bourgois, Antoine  and\n      Poibeau, Thierry\",\n    editor = \"Ogrodniczuk, Maciej  and\n      Novak, Michal  and\n      Poesio, Massimo  and\n      Pradhan, Sameer  and\n      Ng, Vincent\",\n    booktitle = \"Proceedings of the Eighth Workshop on Computational Models of Reference, Anaphora and Coreference\",\n    month = nov,\n    year = \"2025\",\n    address = \"Suzhou, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.crac-1.5/\",\n    doi = \"10.18653/v1/2025.crac-1.5\",\n    pages = \"55--69\",\n    abstract = \"While coreference resolution is attracting more interest than ever from computational literature researchers, representative datasets of fully annotated long documents remain surprisingly scarce. In this paper, we introduce a new annotated corpus of three full-length French novels, totaling over 285,000 tokens. Unlike previous datasets focused on shorter texts, our corpus addresses the challenges posed by long, complex literary works, enabling evaluation of coreference models in the context of long reference chains. We present a modular coreference resolution pipeline that allows for fine-grained error analysis. We show that our approach is competitive and scales effectively to long documents. Finally, we demonstrate its usefulness to infer the gender of fictional characters, showcasing its relevance for both literary analysis and downstream NLP tasks.\"\n}\n</code></pre>","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#english-datasets","level":2,"title":"English Datasets","text":"","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#litbank-en","level":3,"title":"LitBank-en","text":"<p>LitBank is an annotated dataset of 100 works of English-language fiction designed to support tasks in natural language processing and the computational humanities. </p> <p>Note: This version does not modify the underlying annotations, only restructures them for easier use in Propp.</p> Cite this workBibTeX <p>David Bamman, Olivia Lewke, and Anya Mansoor. 2020. An Annotated Dataset of Coreference in English Literature.  In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 44‚Äì54, Marseille, France. European Language Resources Association.</p> <pre><code>@inproceedings{bamman-etal-2020-annotated,\n    title = \"An Annotated Dataset of Coreference in {E}nglish Literature\",\n    author = \"Bamman, David and Lewke, Olivia and Mansoor, Anya\",\n    booktitle = \"Proceedings of the Twelfth Language Resources and Evaluation Conference\",\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2020.lrec-1.6/\",\n    pages = \"44--54\",\n    ISBN = \"979-10-95546-34-4\",\n}\n</code></pre>","path":["Datasets","Available Datasets"],"tags":[]},{"location":"available_datasets/#russian-datasets","level":2,"title":"Russian Datasets","text":"<p>üöß Coming soon... üöß</p>","path":["Datasets","Available Datasets"],"tags":[]},{"location":"character_network/","level":1,"title":"Processing all .txt Files in a Directory","text":"<p>Use this code to generate character network from Propp output files.</p> You can copy / paste the whole Notebook Code <pre><code>from propp_fr import generate_character_network    \n\nfile_name = \"/home/antoine/Bureau/1872_Verne-Jules_Le-tour-du-monde-en-quatre-vingts-jours/1872_Verne-Jules_Le-tour-du-monde-en-quatre-vingts-jours\"\n\nnetwork_metrics_df, G, fig = generate_character_network(\n    file_name = file_name,\n    top_n =10,\n    keep_only_singular=True,\n    save_outputs = True,\n)\n</code></pre>","path":["User Guide","Advanced Processing","Generate Character Network"],"tags":[]},{"location":"conll2003_preprocessing/","level":1,"title":"CoNLL-2023 Preprocessing","text":"<p>Tutorial on how to preprocess the CoNLL-2003 Dataset for Named-Entity Recognition (NER).</p> <p>This dataset should contain:</p> <ul> <li>The raw text files</li> <li>The annotations minimal infos:<ul> <li>The start and end of the annotation (character indexes in the raw text)</li> <li>The label of the annotation (type of entity)</li> </ul> </li> </ul> Python Code to Preprocess the CoNLL-2003 NER Dataset <pre><code>#%%\nimport os\nimport urllib.request\nimport zipfile\n\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm.auto import tqdm\n\nfrom propp_fr import save_text_file, save_entities_df\n\n# URL of the dataset\nurl = \"https://data.deepai.org/conll2003.zip\"\n\n# Destination paths\ndata_dir = \"datasets/conll2003\"\nzip_path = os.path.join(data_dir, \"conll2003.zip\")\n\n# Create directory if it doesn't exist\nos.makedirs(data_dir, exist_ok=True)\n\n# Download the zip file\nprint(\"Downloading dataset...\")\nurllib.request.urlretrieve(url, zip_path)\nprint(\"Download complete!\")\n\n# Unzip\nprint(\"Extracting files...\")\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(data_dir)\nprint(\"Extraction complete!\")\n#%%\ndef generate_split_tokens_df(split_content):\n    doc_ID = -1\n    paragraph_ID = 0\n\n    tokens_df = []\n\n    for row in split_content:\n        row = row.strip()\n        if row == '-DOCSTART- -X- -X- O':\n            doc_ID += 1\n            token_ID = 0\n            paragraph_ID = -1\n        elif row == \"\":\n            paragraph_ID += 1\n        elif row != \"\":\n            word, POS, syntactic, BIO = row.split(\" \")\n            tokens_df.append({\"token_ID\":token_ID,\n                              \"doc_ID\":doc_ID,\n                              \"paragraph_ID\":paragraph_ID,\n                              \"word\":word,\n                              \"POS\":POS,\n                              \"syntactic\":syntactic,\n                              \"BIO\":BIO})\n            token_ID += 1\n\n    tokens_df = pd.DataFrame(tokens_df)\n    return tokens_df\n\ndef extract_conll2003_entities_df(tokens_df):\n    entities = []\n\n    entity = None\n\n    for token_ID, BIO in enumerate(tokens_df[\"BIO\"]):\n\n        if BIO == \"O\":\n            if entity is not None:\n                entity[\"end\"] = token_ID - 1\n                entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                entities.append(entity)\n                entity = None\n            continue\n\n        tag, cat = BIO.split(\"-\", 1)\n\n        if tag == \"B\":\n            if entity is not None:\n                entity[\"end\"] = token_ID - 1\n                entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                entities.append(entity)\n            entity = {\"cat\": cat, \"start\": token_ID}\n\n        elif tag == \"I\":\n            if entity is None or entity[\"cat\"] != cat:\n                # illegal I after O or type mismatch ‚Üí treat as B\n                if entity is not None:\n                    entity[\"end\"] = token_ID - 1\n                    entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                    entities.append(entity)\n                entity = {\"cat\": cat, \"start\": token_ID}\n            # else: valid continuation ‚Üí do nothing\n\n    # close entity at end of document\n    if entity is not None:\n        entity[\"end\"] = len(tokens_df) - 1\n        entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n        entities.append(entity)\n\n    entities_df = pd.DataFrame(entities)\n    return entities_df\n\ndef align_entities_byte_offsets(tokens_df, entities_df):\n    tokens = []\n    reconstructed_text = \"\"\n    token_ID = 0\n    for paragraph_ID, paragraph_tokens_df in tokens_df.groupby(\"paragraph_ID\"):\n        for token_ID_within_sentence, token in enumerate(paragraph_tokens_df[\"word\"].tolist()):\n            byte_onset = len(reconstructed_text)\n            reconstructed_text += str(token)\n            byte_offset = len(reconstructed_text)\n            if token_ID_within_sentence != len(paragraph_tokens_df) - 1:\n                reconstructed_text += \" \"\n            tokens.append({\n                \"paragraph_ID\": paragraph_ID,\n                \"token_ID_within_sentence\":token_ID_within_sentence,\n                \"token_ID\":token_ID,\n                \"byte_onset\": byte_onset,\n                \"byte_offset\": byte_offset,\n            })\n            token_ID += 1\n        if paragraph_ID != len(tokens_df[\"paragraph_ID\"].unique()) - 1:\n            reconstructed_text += \"\\n\"\n\n    tokens_df = pd.DataFrame(tokens)\n    # print(tokens)\n    # Map mention token spans to byte offsets\n    df = pd.merge(entities_df,\n                  tokens_df[[\"token_ID\", \"byte_onset\"]],\n                  left_on=\"start\",\n                  right_on=\"token_ID\",\n                  how=\"left\"\n                  ).drop(columns=[\"token_ID\"])\n\n    df = pd.merge(df,\n                  tokens_df[[\"token_ID\", \"byte_offset\"]],\n                  left_on=\"end\",\n                  right_on=\"token_ID\",\n                  how=\"left\"\n                  ).drop(columns=[\"token_ID\"])\n    entities_df = df.copy()\n    return reconstructed_text, entities_df, tokens_df\n\ndef realign_tokens_offsets(tokens_df, entities_df):\n    start_tokens = []\n    end_tokens = []\n    new_byte_onsets = []\n    new_byte_offsets = []\n\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        start_token = tokens_df[tokens_df[\"byte_offset\"] &gt; mention_byte_onset].index.min()\n        end_token = tokens_df[tokens_df[\"byte_onset\"] &lt; mention_byte_offset].index.max()\n        new_byte_onsets.append(tokens_df.loc[start_token, \"byte_onset\"])\n        new_byte_offsets.append(tokens_df.loc[end_token, \"byte_offset\"])\n\n        start_tokens.append(start_token)\n        end_tokens.append(end_token)\n\n    entities_df[\"start_token\"] = start_tokens\n    entities_df[\"end_token\"] = end_tokens\n    entities_df[\"byte_onset\"] = new_byte_onsets\n    entities_df[\"byte_offset\"] = new_byte_offsets\n\n    return entities_df\n\ndef extract_mention_text(text_content, entities_df):\n    mention_texts = []\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        mention_texts.append(text_content[mention_byte_onset:mention_byte_offset])\n    entities_df[\"text\"] = mention_texts\n    entities_df[\"text\"] = entities_df[\"text\"].astype(str)\n    return entities_df\n#%%\nimport json\n\nfiles_directory = \"datasets/conll2003/conll2003-NER_propp_minimal_implementation\"\n\nsplit_config = {}\n\nfor split_name in [\"train\", \"valid\", \"test\"]:\n    split_config[split_name] = []\n    print(f\"Split: {split_name}\")\n    with open(os.path.join(data_dir, f\"{split_name}.txt\"), \"r\") as f:\n        split_content = f.readlines()\n    tokens_df = generate_split_tokens_df(split_content)\n\n\n    for doc_id, doc_tokens_df in tqdm(tokens_df.groupby(\"doc_ID\")):\n        doc_entities_df = extract_conll2003_entities_df(doc_tokens_df)\n        reconstructed_text, doc_entities_df, doc_tokens_df = align_entities_byte_offsets(doc_tokens_df, doc_entities_df)\n        doc_entities_df = doc_entities_df[[\"cat\", \"byte_onset\", \"byte_offset\"]]\n\n        file_name = f\"{split_name}_{doc_id}\"\n        save_text_file(reconstructed_text, file_name, files_directory)\n        save_entities_df(doc_entities_df, file_name, files_directory)\n\n        split_config[split_name].append(file_name)\n\njson.dump(split_config, open(os.path.join(files_directory, \"split_config.json\"), \"w\"))\n\nimport shutil\n# Create a ZIP archive\nshutil.make_archive(files_directory, 'zip', root_dir=files_directory)\nprint(f\"Archive created: {files_directory}.zip\")\n#%%\n</code></pre> <p>This archive is ready to use with the Propp pipeline.</p> <p>Alternatively, the dataset archive can be downloaded directly from the datasets section.</p>","path":["Training","Dataset Pre Processing","CoNLL-2023 Preprocessing"],"tags":[]},{"location":"contributors/","level":1,"title":"Contributors to the Propp Project","text":"Jean Barr√©    <ul> <li>French Dataset Annotations</li> <li>Downstream Applications</li> </ul>      Antoine Bourgois    <ul> <li>Main Developer</li> <li>Python Library</li> <li>French Dataset Annotations</li> <li>Model Training and Evaluation</li> </ul>      Thierry Poibeau    <ul> <li>Project Supervisor</li> </ul>","path":["Contributors"],"tags":[]},{"location":"contributors/#acknowledgements","level":1,"title":"Acknowledgements","text":"","path":["Contributors"],"tags":[]},{"location":"datasets_benchmarks/","level":1,"title":"Datasets Benchmarks","text":"","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#ner-datasets","level":2,"title":"NER datasets","text":"","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#conll2003-ner","level":3,"title":"conll2003-NER","text":"<p>Test Splits: 1 [231 File(s) / split]  |  Overall Tested Ratio: 16.58% [231/1393 Files]</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 89.59 88.07 5,648 google-bert/bert-base-cased 90.95 89.28 5,648 google-bert/bert-large-cased 91.51 89.81 5,648 FacebookAI/xlm-roberta-large 92.75 91.26 5,648 FacebookAI/roberta-large 93.05 91.45 5,648 google/t5-v1_1-xl 93.39 91.85 5,648 google/flan-t5-xl 93.57 92.38 5,648 google/mt5-xl 93.70 92.32 5,648","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#litbank-fr","level":3,"title":"litbank-fr","text":"<p>Test Splits: 10 [10 File(s) / split]  |  Overall Tested Ratio: 100.00% [100/100 Files]</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 86.09 58.90 29,103 google-bert/bert-base-cased 87.62 62.08 29,103 google-bert/bert-large-cased 87.93 64.56 29,103 google/t5-v1_1-xl 88.65 66.27 29,103 FacebookAI/xlm-roberta-large 88.70 66.69 29,103 FacebookAI/roberta-large 88.85 67.02 29,103 google/flan-t5-xl 88.96 66.24 29,103 google/mt5-xl 89.00 65.68 29,103 <p>Test Splits: 29 [1 File(s) / split]  |  Overall Tested Ratio: 100.00% [29/29 Files]</p> Embedding Model Micro F1 Macro F1 Support almanach/moderncamembert-cv2-base 84.43 54.84 38,630 almanach/moderncamembert-base 85.31 57.20 38,630 FacebookAI/xlm-roberta-large 87.70 60.39 38,630 almanach/camembert-base 87.76 61.43 38,630 google/mt5-xl 88.16 61.62 38,630 almanach/camembert-large 88.19 62.43 38,630","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#long-litbank-fr-per-only","level":3,"title":"long-litbank-fr-PER-only","text":"<p>Test Splits: 32 [1 File(s) / split]  |  Overall Tested Ratio: 100.00% [32/32 Files]</p> Embedding Model Micro F1 Macro F1 Support almanach/moderncamembert-cv2-base 91.94 91.94 71,883 almanach/moderncamembert-base 92.74 92.74 71,883 FacebookAI/xlm-roberta-large 94.45 94.45 71,883 almanach/camembert-base 94.57 94.57 71,883 almanach/camembert-large 94.74 94.74 71,883 google/mt5-xl 94.74 94.74 71,883","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#ontonotes5_english-ner","level":3,"title":"ontonotes5_english-NER","text":"<p>Test Splits: 1 [207 File(s) / split]  |  Overall Tested Ratio: 5.69% [207/3637 Files]</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 88.66 78.53 11,257 google-bert/bert-large-cased 89.40 79.53 11,257 google-bert/bert-base-cased 89.42 80.20 11,257 google/flan-t5-xl 90.52 81.81 11,257 FacebookAI/xlm-roberta-large 90.59 81.32 11,257 google/mt5-xl 90.66 82.30 11,257 google/t5-v1_1-xl 90.74 82.05 11,257 FacebookAI/roberta-large 90.84 81.96 11,257 <p>=============</p> <p>conll2003-NER Mention Spans Detection (test set)</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 89.59 88.07 5,648 google-bert/bert-base-cased 90.95 89.28 5,648 google-bert/bert-large-cased 91.51 89.81 5,648 FacebookAI/xlm-roberta-large 92.75 91.26 5,648 FacebookAI/roberta-large 93.05 91.45 5,648 google/t5-v1_1-xl 93.39 91.85 5,648 google/mt5-xxl 93.54 92.22 5,648 google/flan-t5-xl 93.57 92.38 5,648 google/mt5-xl 93.70 92.32 5,648 <p>OntoNotes 5 - NER Mention Spans Detection (test set)</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 88.11 77.26 11,257 google-bert/bert-base-cased 89.13 80.03 11,257 google-bert/bert-large-cased 89.31 79.11 11,257 google/flan-t5-xl 89.67 79.57 11,257 FacebookAI/roberta-large 90.55 81.96 11,257 google/mt5-xl 90.58 82.35 11,257 FacebookAI/xlm-roberta-large 90.60 81.63 11,257","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#litbank-fr_1","level":3,"title":"LitBank-fr","text":"","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#long-litbank-fr-characters-only","level":3,"title":"Long-LitBank-fr (characters only)","text":"<p>We evaluated PROPP‚Äôs NER pipeline using multiple transformer-based embedding models. (LOOCV) <code>PER</code> mentions only</p> Embedding Model Micro F1 Macro F1 Support almanach/moderncamembert-cv2-base 92.08 92.08 71,883 dbmdz/bert-base-french-europeana-cased 93.95 93.95 71,883 FacebookAI/xlm-roberta-large 94.40 94.40 71,883 almanach/camembert-base 94.53 94.53 71,883 google/mt5-xl 94.56 94.56 71,883 almanach/camembert-large 94.68 94.68 71,883","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#litbank-en","level":3,"title":"LitBank-en","text":"<p>We evaluated PROPP‚Äôs NER pipeline using multiple transformer-based embedding models. (test set 0)</p> Embedding Model Micro F1 Macro F1 Support answerdotai/ModernBERT-large 85.65 55.50 2,832 google-bert/bert-base-cased 87.12 60.96 2,832 google-bert/bert-large-cased 87.26 63.58 2,832 google/mt5-xxl 87.86 60.89 2,832 FacebookAI/xlm-roberta-large 88.04 61.49 2,832 FacebookAI/roberta-large 88.08 61.67 2,832 google/t5-v1_1-xl 88.22 63.26 2,832 google/flan-t5-xl 88.53 61.95 2,832 google/mt5-xl 89.10 68.74 2,832","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"datasets_benchmarks/#coreference-resolution-datasets","level":2,"title":"Coreference Resolution datasets","text":"<p>litbank_propp_minimal_implementation Coreference Resolution (Gold mentions)</p> Embedding Model MUC B3 CEAFe CONLL Avg. Tokens Count Avg. Mentions Count Doc. FacebookAI/xlm-roberta-large 88.05 77.00 76.67 80.57 2136 291 100 google-bert/bert-base-cased 86.00 72.93 74.03 77.66 2136 291 100 google-bert/bert-large-cased 86.53 74.94 75.08 78.85 2136 291 100 google/mt5-xl 89.63 80.61 78.76 83.00 2136 291 100","path":["Datasets","Datasets Benchmarks"],"tags":[]},{"location":"litbank_preprocessing/","level":1,"title":"LitBank Preprocessing","text":"<p>Tutorial on how to preprocess the LitBank Dataset for coreference resolution.</p> <p>This dataset should contain:</p> <ul> <li>The raw text files</li> <li>The annotations minimal infos:<ul> <li>The start and end of the annotation (character indexes in the raw text)</li> <li>The label of the annotation (type of entity)</li> <li>The coreference chains ID  </li> </ul> </li> </ul> <p>We first need to download the dataset from the github repository: https://github.com/dbamman/litbank/tree/master/coref/brat</p> Python Code to Download the Dataset <pre><code>import os\n\nlocal_dataset_path = \"datasets/litbank\"\nos.makedirs(dataset_path, exist_ok=True)\n\n# The specific GitHub folder you want\ngithub_folder_path = \"coref/brat\"\nrepo_url = \"https://github.com/dbamman/litbank.git\"\n\n# Run the git commands (note the exclamation mark !)\n!cd {dataset_path} &amp;&amp; git init\n!cd {dataset_path} &amp;&amp; git remote add origin -f {repo_url}\n!cd {dataset_path} &amp;&amp; git config core.sparseCheckout true\n!cd {dataset_path} &amp;&amp; echo \"{github_folder_path}/*\" &gt;&gt; .git/info/sparse-checkout\n!cd {dataset_path} &amp;&amp; git pull origin master\n# Reorganize: move the brat directory to the root of dataset_path, then remove coref and .git\n!cd {dataset_path} &amp;&amp; mv coref/brat . &amp;&amp; rm -rf coref &amp;&amp; rm -rf .git\n</code></pre> <p>We can now take a look at the dataset:</p> Python Code <pre><code>from pathlib import Path\nfrom collections import Counter\n\nraw_files_path = \"datasets/litbank/tsv\"\n\nall_files = sorted([p for p in Path(raw_files_path).iterdir() if p.is_file()])\n\nextensions = dict(Counter(p.suffix for p in files).most_common())\nprint(f\"The dataset contains {len(all_files):,} files with the following extensions:\")\nfor extension, count in extensions.items():\n    print(f\"\\t'{extension}' = {count:,}\")\n</code></pre> <pre><code>The dataset contains 200 files with the following extensions:\n    '.txt' = 100\n    '.ann' = 100\n</code></pre> <p>While the <code>.txt</code> file contains the raw text, we need to take a closer look at the <code>.ann</code> file.</p> Python Code <pre><code>file_name = all_files[0].stem\n\ntxt_files = [p for p in all_files if p.suffix == \".txt\"]\nwith open(ann_files[0], \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n\nann_files = [p for p in all_files if p.suffix == \".ann\"]\nann_df = pd.read_csv(ann_files[0], sep=\"\\t\", header=None)\n\nprint(ann_df.sample(5).to_markdown())\n</code></pre> 0 1 2 3 4 5 6 7 8 393 COREF T174 Ralph_Percy-0 nan nan nan nan nan nan 18 MENTION T27 15 45 15 46 my house FAC NOM 42 MENTION T65 70 4 70 4 wife PER NOM 335 COREF T135 settlers-7 nan nan nan nan nan nan 245 MENTION T254 51 0 51 0 I PER PRON <p>We see that the first column contains the annotation type, on which the content of other columns depends.</p> Python Code <pre><code>type_to_description_mapping = {\n    \"MENTION\": \"Entity mention with span and semantic type\",\n    \"COREF\": \"True coreference link between two mentions\",\n    \"COP\": \"Copular relation between two mentions (not treated as coreference)\",\n    \"APPOS\": \"Appositional relation between two mentions (not treated as coreference)\",\n}\n\nrow_types_df = pd.DataFrame(\n        Counter(ann_df[0]).most_common(),\n        columns=[\"Column '0' Type\", \"Count\"]\n    )\nrow_types_df[\"Row Type\"] = row_types_df[\"Column '0' Type\"].map(type_to_description_mapping)\n\nprint(row_types_df.to_markdown(index=False))\n</code></pre> Column '0' Type Count Row Type MENTION 316 Entity mention COREF 307 True coreference link between two mentions COP 6 Copular relation between two mentions (not treated as coreference) APPOS 3 Appositional relation between two mentions (not treated as coreference) <p>In this case, only the <code>MENTION</code> and <code>COREF</code> annotations are relevant, as copulae and appositions are not considered strict coreference relations.</p> <p>We merge <code>MENTION</code> and <code>COREF</code> annotations into a single table and assign unique coreference IDs to singleton mentions:</p> Python Code <pre><code># Filter mentions\nmention_df = ann_df[ann_df[0]==\"MENTION\"].reset_index(drop=True)\nmention_df = mention_df[[1,2,3,5,6,7,8]]\nmention_df.columns = [\"mention_ID\",\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\",\"text\",\"cat\",\"prop\"]\nmention_df[[\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"]] = mention_df[[\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"]].astype(int)\n\n# Filter coreference links\ncoref_df = ann_df[ann_df[0]==\"COREF\"].reset_index(drop=True)\ncoref_df = coref_df[[1,2]]\ncoref_df.columns = [\"mention_ID\",\"COREF_name\"]\n\n# Merge mentions with coreference info\nentities_df = pd.merge(coref_df, mention_df, on=\"mention_ID\", how=\"outer\")\\\n                 .drop(columns=[\"mention_ID\"])\\\n                 .sort_values([\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"])\\\n                 .reset_index(drop=True)\n\n# Fill missing COREF_name with unique names\nexisting = set(entities_df[\"COREF_name\"].dropna())\nfor idx, row in entities_df[entities_df[\"COREF_name\"].isna()].iterrows():\n    base = row[\"text\"].replace(\" \", \"_\")\n    i = 1\n    name = base\n    while name in existing:\n        i += 1\n        name = f\"{base}_{i}\"\n    entities_df.at[idx, \"COREF_name\"] = name\n    existing.add(name)\n</code></pre> <p>We split the original text into paragraphs and tokens, compute tokens byte offsets, and directly merge them with the mentions to map token spans to text offsets.</p> Python Code <pre><code># Split text into tokens and track byte offsets\ntokens = []\nreconstructed_text = \"\"\nfor paragraph_ID, paragraph in enumerate(text_content.split(\"\\n\")):\n    if paragraph_ID != 0:\n        reconstructed_text += \"\\n\"\n    for token_ID_within_sentence, token in enumerate(paragraph.split(\" \")):\n        byte_onset = len(reconstructed_text)\n        reconstructed_text += token\n        byte_offset = len(reconstructed_text)\n        if token_ID_within_sentence != len(paragraph.split(\" \")) - 1:\n            reconstructed_text += \" \"\n        tokens.append({\n            \"paragraph_ID\": paragraph_ID,\n            \"token_ID_within_sentence\":token_ID_within_sentence,\n            \"byte_onset\": byte_onset,\n            \"byte_offset\": byte_offset,\n        })\n\ntokens_df = pd.DataFrame(tokens)\n\n# Map mention token spans to byte offsets\ndf = pd.merge(entities_df,\n              tokens_df[[\"paragraph_ID\", \"token_ID_within_sentence\", \"byte_onset\"]],\n              left_on=[\"paragraph_ID\", \"start_token_within_sentence\"],\n              right_on=[\"paragraph_ID\", \"token_ID_within_sentence\"],\n              ).drop(columns=[\"token_ID_within_sentence\"])\ndf = pd.merge(df,\n              tokens_df[[\"paragraph_ID\", \"token_ID_within_sentence\",\"byte_offset\"]],\n              left_on=[\"paragraph_ID\", \"end_token_within_sentence\"],\n              right_on=[\"paragraph_ID\", \"token_ID_within_sentence\"],\n              ).drop(columns=[\"token_ID_within_sentence\"])\nentities_df = df.copy()\n</code></pre> <p>We now have two key objects for downstream coreference tasks:</p> <ul> <li><code>reconstructed_text</code>: the original text stripped of trailing spaces and formatted with simple <code>\\n</code> between paragraphs.</li> <li><code>entities_df</code>: a <code>pandas.dataframe</code> containing all annotated mentions with byte offsets aligned to reconstructed_text and associated coreference information.  </li> </ul> <p>The minimal configuration of <code>entities_df</code> needed to train a coreference resolution pipeline is:</p> <ul> <li><code>byte_onset</code>: exact character index of the mention start in <code>reconstructed_text</code>,</li> <li><code>byte_offset</code>: exact character index of the mention end in <code>reconstructed_text</code>,</li> <li><code>cat</code>: the mention type (e.g. <code>PER</code>, <code>FAC</code>, <code>GPE</code>, <code>ORG</code>),</li> <li><code>COREF_name</code>: a unique identifier for the mention coreference chain.  </li> </ul> <p>In this case <code>entities_df</code> contains additional columns:</p> <ul> <li><code>text</code>: the surface form of the mention,</li> <li><code>prop</code>: the manually annotated grammatical type of the mention (proper, noun phrase, pronoun).</li> </ul> byte_onset byte_offset cat COREF_name text prop 13 21 FAC Chancery-0 Chancery PROP 22 28 GPE London-1 London PROP 69 84 PER Lord_Chancellor-2 Lord Chancellor PROP 96 115 FAC Lincoln__s_Inn_Hall-3 Lincoln 's Inn Hall PROP 163 174 FAC the_streets-4 the streets NOM <p>We can now save our formatted <code>reconstructed_text</code> and <code>entities_df</code> to our dataset directory:</p> Python Code <pre><code>from propp_fr import save_text_file, save_entities_df\n\nlocal_dataset_path = \"datasets/litbank\"\nos.makedirs(local_dataset_path, exist_ok=True)\n\nsave_text_file(reconstructed_text, file_name, local_dataset_path)\nsave_entities_df(entities_df, file_name, local_dataset_path)\n</code></pre> <p>Finally, we can wrap the entire preprocessing pipeline into a loop to handle all 100 LitBank files.</p> Python Code <pre><code>from tqdm.auto import tqdm\nimport pandas as pd\nfrom pathlib import Path\nimport os\nfrom propp_fr import save_text_file, save_entities_df\n\nraw_files_directory_path = \"datasets/litbank/tsv\"\nlocal_dataset_path = \"datasets/litbank/propp_minimal\"\n\nfile_names = sorted(list(set([p.stem for p in Path(raw_files_directory_path).iterdir() if p.is_file()])))\n\nfor file_name in tqdm(file_names):\n    # Loading Input Files\n    with open(os.path.join(raw_files_directory_path, file_name + \".txt\"), \"r\", encoding=\"utf-8\") as f:\n        text_content = f.read()\n    ann_df = pd.read_csv(os.path.join(raw_files_directory_path, file_name + \".ann\"), sep=\"\\t\", header=None)\n\n    # Filter mentions\n    mention_df = ann_df[ann_df[0]==\"MENTION\"].reset_index(drop=True)\n    mention_df = mention_df[[1,2,3,5,6,7,8]]\n    mention_df.columns = [\"mention_ID\",\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\",\"text\",\"cat\",\"prop\"]\n    mention_df[[\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"]] = mention_df[[\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"]].astype(int)\n\n    # Filter coreference links\n    coref_df = ann_df[ann_df[0]==\"COREF\"].reset_index(drop=True)\n    coref_df = coref_df[[1,2]]\n    coref_df.columns = [\"mention_ID\",\"COREF_name\"]\n\n    # Merge mentions with coreference info\n    entities_df = pd.merge(coref_df, mention_df, on=\"mention_ID\", how=\"outer\")\\\n                     .drop(columns=[\"mention_ID\"])\\\n                     .sort_values([\"paragraph_ID\",\"start_token_within_sentence\",\"end_token_within_sentence\"])\\\n                     .reset_index(drop=True)\n\n    # Fill missing COREF_name with unique names\n    existing = set(entities_df[\"COREF_name\"].dropna())\n    for idx, row in entities_df[entities_df[\"COREF_name\"].isna()].iterrows():\n        base = row[\"text\"].replace(\" \", \"_\")\n        i = 1\n        name = base\n        while name in existing:\n            i += 1\n            name = f\"{base}_{i}\"\n        entities_df.at[idx, \"COREF_name\"] = name\n        existing.add(name)\n\n    # Split text into tokens and track byte offsets\n    tokens = []\n    reconstructed_text = \"\"\n    for paragraph_ID, paragraph in enumerate(text_content.split(\"\\n\")):\n        if paragraph_ID != 0:\n            reconstructed_text += \"\\n\"\n        for token_ID_within_sentence, token in enumerate(paragraph.split(\" \")):\n            byte_onset = len(reconstructed_text)\n            reconstructed_text += token\n            byte_offset = len(reconstructed_text)\n            if token_ID_within_sentence != len(paragraph.split(\" \")) - 1:\n                reconstructed_text += \" \"\n            tokens.append({\n                \"paragraph_ID\": paragraph_ID,\n                \"token_ID_within_sentence\":token_ID_within_sentence,\n                \"byte_onset\": byte_onset,\n                \"byte_offset\": byte_offset,\n            })\n\n    tokens_df = pd.DataFrame(tokens)\n\n    # Map mention token spans to byte offsets\n    df = pd.merge(entities_df,\n                  tokens_df[[\"paragraph_ID\", \"token_ID_within_sentence\", \"byte_onset\"]],\n                  left_on=[\"paragraph_ID\", \"start_token_within_sentence\"],\n                  right_on=[\"paragraph_ID\", \"token_ID_within_sentence\"],\n                  ).drop(columns=[\"token_ID_within_sentence\"])\n    df = pd.merge(df,\n                  tokens_df[[\"paragraph_ID\", \"token_ID_within_sentence\",\"byte_offset\"]],\n                  left_on=[\"paragraph_ID\", \"end_token_within_sentence\"],\n                  right_on=[\"paragraph_ID\", \"token_ID_within_sentence\"],\n                  ).drop(columns=[\"token_ID_within_sentence\", \"paragraph_ID\", \"start_token_within_sentence\", \"end_token_within_sentence\"])\n    entities_df = df.copy()\n\n    minimal_columns = ['byte_onset', 'byte_offset', 'cat', 'COREF_name']\n    entities_df = entities_df[minimal_columns + [col for col in df.columns if col not in minimal_columns]]\n\n    file_name = file_name.replace(\"_brat\", \"\") if file_name.endswith(\"_brat\") else file_name\n    save_text_file(reconstructed_text, file_name, local_dataset_path)\n    save_entities_df(entities_df, file_name, local_dataset_path)\n\nimport requests\n\nsplit_id = 0\nsplit_config = {}\n\nwhile True:\n    url = f\"https://raw.githubusercontent.com/dbamman/lrec2020-coref/master/data/litbank_tenfold_splits/{split_id}/test.ids\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException:\n        # No more splits (404 or network error)\n        break\n\n    content = response.text\n\n    split_content = [\n        file_name.replace(\"_brat.tsv\", \"\")\n        for file_name in content.split(\"\\n\")\n        if file_name.endswith(\"_brat.tsv\")\n    ]\n\n    split_config[f\"test_{split_id}\"] = split_content\n    split_id += 1\n\nimport json\njson.dump(split_config, open(os.path.join(local_dataset_path, \"split_config.json\"), \"w\"))\n\nimport shutil\n\noutput_archive_name = \"litbank_propp_minimal_implementation\"\n# Path where the ZIP will be saved (same level as local_dataset_path)\noutput_path = os.path.join(os.path.dirname(local_dataset_path), output_archive_name)\n# Create a ZIP archive\nshutil.make_archive(output_path, 'zip', root_dir=local_dataset_path)\nprint(f\"Archive created: {output_path}.zip\")\n</code></pre> <p>Finally, the script generates a compressed ZIP archive of the processed dataset:</p> <pre><code>litbank_propp_minimal_implementation.zip\n</code></pre> <p>This archive is ready to use with the Propp pipeline.</p> <p>Alternatively, the dataset archive can be downloaded directly from the datasets section.</p>","path":["Training","Dataset Pre Processing","LitBank Preprocessing"],"tags":[]},{"location":"ontonotes5_preprocessing/","level":1,"title":"OntoNotes 5 Preprocessing","text":"<p>Tutorial on how to preprocess the OntoNotes 5.0 Dataset for Named-Entity Recognition (NER).</p> <p>This Dataset is subject to law and can't be shared freely, we will not share the dataset neither raw or after preprocessing, this tutorial can be used by anyone that obtained access to the dataset by Linguistic Data Consortium. It is free. It is easy you just need to follow the tutorial. Licensing Instructions:    Subscription &amp; Standard Members, and Non-Members</p> <p>Start with a directory containing the <code>ontonotes-release-5.0_LDC2013T19.tgz</code> archive.</p> Python Code to Preprocess the CoNLL-2003 NER Dataset <pre><code>#%%\nimport os\nimport urllib.request\nimport zipfile\n\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm.auto import tqdm\n\nfrom propp_fr import save_text_file, save_entities_df\n\n# URL of the dataset\nurl = \"https://data.deepai.org/conll2003.zip\"\n\n# Destination paths\ndata_dir = \"datasets/conll2003\"\nzip_path = os.path.join(data_dir, \"conll2003.zip\")\n\n# Create directory if it doesn't exist\nos.makedirs(data_dir, exist_ok=True)\n\n# Download the zip file\nprint(\"Downloading dataset...\")\nurllib.request.urlretrieve(url, zip_path)\nprint(\"Download complete!\")\n\n# Unzip\nprint(\"Extracting files...\")\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(data_dir)\nprint(\"Extraction complete!\")\n#%%\ndef generate_split_tokens_df(split_content):\n    doc_ID = -1\n    paragraph_ID = 0\n\n    tokens_df = []\n\n    for row in split_content:\n        row = row.strip()\n        if row == '-DOCSTART- -X- -X- O':\n            doc_ID += 1\n            token_ID = 0\n            paragraph_ID = -1\n        elif row == \"\":\n            paragraph_ID += 1\n        elif row != \"\":\n            word, POS, syntactic, BIO = row.split(\" \")\n            tokens_df.append({\"token_ID\":token_ID,\n                              \"doc_ID\":doc_ID,\n                              \"paragraph_ID\":paragraph_ID,\n                              \"word\":word,\n                              \"POS\":POS,\n                              \"syntactic\":syntactic,\n                              \"BIO\":BIO})\n            token_ID += 1\n\n    tokens_df = pd.DataFrame(tokens_df)\n    return tokens_df\n\ndef extract_conll2003_entities_df(tokens_df):\n    entities = []\n\n    entity = None\n\n    for token_ID, BIO in enumerate(tokens_df[\"BIO\"]):\n\n        if BIO == \"O\":\n            if entity is not None:\n                entity[\"end\"] = token_ID - 1\n                entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                entities.append(entity)\n                entity = None\n            continue\n\n        tag, cat = BIO.split(\"-\", 1)\n\n        if tag == \"B\":\n            if entity is not None:\n                entity[\"end\"] = token_ID - 1\n                entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                entities.append(entity)\n            entity = {\"cat\": cat, \"start\": token_ID}\n\n        elif tag == \"I\":\n            if entity is None or entity[\"cat\"] != cat:\n                # illegal I after O or type mismatch ‚Üí treat as B\n                if entity is not None:\n                    entity[\"end\"] = token_ID - 1\n                    entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n                    entities.append(entity)\n                entity = {\"cat\": cat, \"start\": token_ID}\n            # else: valid continuation ‚Üí do nothing\n\n    # close entity at end of document\n    if entity is not None:\n        entity[\"end\"] = len(tokens_df) - 1\n        entity[\"len\"] = entity[\"end\"] - entity[\"start\"] + 1\n        entities.append(entity)\n\n    entities_df = pd.DataFrame(entities)\n    return entities_df\n\ndef align_entities_byte_offsets(tokens_df, entities_df):\n    tokens = []\n    reconstructed_text = \"\"\n    token_ID = 0\n    for paragraph_ID, paragraph_tokens_df in tokens_df.groupby(\"paragraph_ID\"):\n        for token_ID_within_sentence, token in enumerate(paragraph_tokens_df[\"word\"].tolist()):\n            byte_onset = len(reconstructed_text)\n            reconstructed_text += str(token)\n            byte_offset = len(reconstructed_text)\n            if token_ID_within_sentence != len(paragraph_tokens_df) - 1:\n                reconstructed_text += \" \"\n            tokens.append({\n                \"paragraph_ID\": paragraph_ID,\n                \"token_ID_within_sentence\":token_ID_within_sentence,\n                \"token_ID\":token_ID,\n                \"byte_onset\": byte_onset,\n                \"byte_offset\": byte_offset,\n            })\n            token_ID += 1\n        if paragraph_ID != len(tokens_df[\"paragraph_ID\"].unique()) - 1:\n            reconstructed_text += \"\\n\"\n\n    tokens_df = pd.DataFrame(tokens)\n    # print(tokens)\n    # Map mention token spans to byte offsets\n    df = pd.merge(entities_df,\n                  tokens_df[[\"token_ID\", \"byte_onset\"]],\n                  left_on=\"start\",\n                  right_on=\"token_ID\",\n                  how=\"left\"\n                  ).drop(columns=[\"token_ID\"])\n\n    df = pd.merge(df,\n                  tokens_df[[\"token_ID\", \"byte_offset\"]],\n                  left_on=\"end\",\n                  right_on=\"token_ID\",\n                  how=\"left\"\n                  ).drop(columns=[\"token_ID\"])\n    entities_df = df.copy()\n    return reconstructed_text, entities_df, tokens_df\n\ndef realign_tokens_offsets(tokens_df, entities_df):\n    start_tokens = []\n    end_tokens = []\n    new_byte_onsets = []\n    new_byte_offsets = []\n\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        start_token = tokens_df[tokens_df[\"byte_offset\"] &gt; mention_byte_onset].index.min()\n        end_token = tokens_df[tokens_df[\"byte_onset\"] &lt; mention_byte_offset].index.max()\n        new_byte_onsets.append(tokens_df.loc[start_token, \"byte_onset\"])\n        new_byte_offsets.append(tokens_df.loc[end_token, \"byte_offset\"])\n\n        start_tokens.append(start_token)\n        end_tokens.append(end_token)\n\n    entities_df[\"start_token\"] = start_tokens\n    entities_df[\"end_token\"] = end_tokens\n    entities_df[\"byte_onset\"] = new_byte_onsets\n    entities_df[\"byte_offset\"] = new_byte_offsets\n\n    return entities_df\n\ndef extract_mention_text(text_content, entities_df):\n    mention_texts = []\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        mention_texts.append(text_content[mention_byte_onset:mention_byte_offset])\n    entities_df[\"text\"] = mention_texts\n    entities_df[\"text\"] = entities_df[\"text\"].astype(str)\n    return entities_df\n#%%\nimport json\n\nfiles_directory = \"datasets/conll2003/conll2003-NER_propp_minimal_implementation\"\n\nsplit_config = {}\n\nfor split_name in [\"train\", \"valid\", \"test\"]:\n    split_config[split_name] = []\n    print(f\"Split: {split_name}\")\n    with open(os.path.join(data_dir, f\"{split_name}.txt\"), \"r\") as f:\n        split_content = f.readlines()\n    tokens_df = generate_split_tokens_df(split_content)\n\n\n    for doc_id, doc_tokens_df in tqdm(tokens_df.groupby(\"doc_ID\")):\n        doc_entities_df = extract_conll2003_entities_df(doc_tokens_df)\n        reconstructed_text, doc_entities_df, doc_tokens_df = align_entities_byte_offsets(doc_tokens_df, doc_entities_df)\n        doc_entities_df = doc_entities_df[[\"cat\", \"byte_onset\", \"byte_offset\"]]\n\n        file_name = f\"{split_name}_{doc_id}\"\n        save_text_file(reconstructed_text, file_name, files_directory)\n        save_entities_df(doc_entities_df, file_name, files_directory)\n\n        split_config[split_name].append(file_name)\n\njson.dump(split_config, open(os.path.join(files_directory, \"split_config.json\"), \"w\"))\n\nimport shutil\n# Create a ZIP archive\nshutil.make_archive(files_directory, 'zip', root_dir=files_directory)\nprint(f\"Archive created: {files_directory}.zip\")\n#%%\n</code></pre> <p>This archive is ready to use with the Propp pipeline.</p> <p>Alternatively, the dataset archive can be downloaded directly from the datasets section.</p>","path":["OntoNotes 5 Preprocessing"],"tags":[]},{"location":"processing_all_directory/","level":1,"title":"Processing all .txt Files in a Directory","text":"<p>This code will process all the <code>.txt</code> files in a <code>files_directory</code> and saves the results (<code>tokens_df</code>, <code>entities_df</code>, and <code>characters_dict</code>).</p> You can copy / paste the whole Notebook Code <pre><code>from propp_fr import load_models, load_text_file, generate_tokens_df, load_tokenizer_and_embedding_model, get_embedding_tensor_from_tokens_df, generate_entities_df, add_features_to_entities, perform_coreference, extract_attributes, generate_characters_dict, save_tokens_df, save_entities_df, save_book_file\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfiles_directory = #\"&lt;directory_containing_txt_files&gt;\"\n\ntxt_files = sorted(p.stem for p in Path(files_directory).iterdir() if p.suffix == \".txt\")\nbook_files = sorted(p.stem for p in Path(files_directory).iterdir() if p.suffix == \".book\")\n\nunprocessed_files = [file for file in txt_files if file not in book_files]\nprint(f\"Unprocessed Files: {len(unprocessed_files):,}\")\n\nspacy_model, mentions_detection_model, coreference_resolution_model = load_models()\ntokenizer, embedding_model = load_tokenizer_and_embedding_model(mentions_detection_model[\"base_model_name\"])\n\nfor file_name in tqdm(unprocessed_files, desc=\"Processing .txt Files\"):\n    print(f\"Processing: {file_name}...\")\n    text_content = load_text_file(file_name, files_directory)\n    tokens_df = generate_tokens_df(text_content, spacy_model)\n    tokens_embedding_tensor = get_embedding_tensor_from_tokens_df(\n        text_content,\n        tokens_df,\n        tokenizer,\n        embedding_model\n    )\n\n    entities_df = generate_entities_df(\n        tokens_df,\n        tokens_embedding_tensor,\n        mentions_detection_model,\n    )\n\n    entities_df = add_features_to_entities(entities_df, tokens_df)\n\n    entities_df = perform_coreference(\n        entities_df,\n        tokens_embedding_tensor,\n        coreference_resolution_model,\n        propagate_coref=True,\n        rule_based_postprocess=False,\n        characters_alias_list=None, # Can pass a list of lists, or a dict with values being list of aliases\n                                    # characters_alias_list = {\n                                    #                     'Phileas_Fogg': [\"phileas fogg\", \"mr. fogg\", \"mr. phileas fogg\", \"m. phileas fogg\",],\n                                    #                     'Jean_Passepartout': [\"passepartout\"],\n                                    #                     'Fix': [\"fix\", \"l' inspecteur fix\", \"monsieur fix\",],\n                                    #                     'Mrs_Aouda': [\"mrs . aouda\"],\n                                    #                     'James_Forster': [\"james forster\"],}\n\n    )\n\n    tokens_df = extract_attributes(entities_df, tokens_df)\n\n    characters_dict = generate_characters_dict(tokens_df, entities_df)\n\n\n    save_tokens_df(tokens_df, file_name, files_directory)\n    save_entities_df(entities_df, file_name, files_directory)\n    save_book_file(characters_dict, file_name, files_directory)\n</code></pre>","path":["User Guide","Basic Processing","Processing Full Directory"],"tags":[]},{"location":"quick_start/","level":1,"title":"Quick Start","text":"","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#google-colab-hands-on-tutorial","level":2,"title":"Google Colab Hands-on Tutorial","text":"<p><code>This Notebook</code> will guide you through the process of analyzing a French novel using the propp-fr library. </p> <p>You'll learn how to load a novel, tokenize it, extract named entities, resolve coreferences, and analyze the main characters.</p>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#installation","level":2,"title":"Installation","text":"<p>The French variant of the Propp python library can be installed via pypi:</p> <pre><code>pip install propp_fr\n</code></pre>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#oneliner-processing","level":2,"title":"Oneliner Processing","text":"<p>You can process a text file in one line with the default models:</p> <pre><code>from propp_fr import process_text_file\n\nprocess_text_file(\"root_directory/my_french_novel.txt\")\n</code></pre> <p>This will generate three additional files in the same directory:</p> <pre><code>root_directory/\n‚îú‚îÄ‚îÄ my_french_novel.txt\n‚îú‚îÄ‚îÄ my_french_novel.tokens\n‚îú‚îÄ‚îÄ my_french_novel.entities\n‚îî‚îÄ‚îÄ my_french_novel.book\n</code></pre> <ul> <li> <p><code>my_french_novel.tokens</code> contains all tokens along with:</p> <ul> <li>Part-of-speech tags</li> <li>Syntactic parsing information  </li> </ul> </li> <li> <p><code>my_french_novel.entities</code> contains information about recognized entities, including:</p> <ul> <li>Start and end positions  </li> <li>Entity type  </li> </ul> </li> <li> <p><code>my_french_novel.book</code> contains all characters and their attributes, including:</p> <ul> <li>Coreference information  </li> <li>Gender, number, and other features  </li> </ul> </li> </ul>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#reloading-processed-files","level":2,"title":"Reloading Processed Files","text":"<p>Generated files can be loaded by:</p> <pre><code>from propp_fr import load_text_file, load_tokens_df, load_entities_df, load_book_file\n\nfile_name = \"my_french_novel\"\nroot_directory = \"root_directory\"\n\ntext_content = load_text_file(file_name, root_directory)\ntokens_df = load_tokens_df(file_name, root_directory)\nentities_df = load_entities_df(file_name, root_directory)\ncharacters_dict = load_book_file(file_name, root_directory)\n</code></pre>","path":["User Guide","Quick Start"],"tags":[]},{"location":"quick_start/#yes-no-flowchart","level":2,"title":"Yes No Flowchart","text":"<p>Did you define your entity types (annotation guidelines)?   -&gt; No -&gt; Annotation Guidelines Is there an annotated dataset containing those entities? (See Available Dataset)   -&gt; No -&gt; Annotate Dataset In the right language?   -&gt; No -&gt; Annotate Dataset (at least test set to evaluate model) OR Transfer Annotations OR Train a multilingual model Is there a pretrained model available? -&gt; Is performance acceptable? (see Dataset Benchmarks) Yes -&gt; Use the pretrained model No -&gt; Train and Evaluate a new model (how to improve model section: ablation, dataset size (tokens, mentions, monoentity), embedding model, model architecture)</p>","path":["User Guide","Quick Start"],"tags":[]},{"location":"sacr_dataset_annotation/","level":1,"title":"Annotating a new dataset","text":"","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#why-annotating-a-new-dataset","level":2,"title":"Why Annotating a New Dataset ?","text":"<p>Sometimes the entity types you are interested in are not covered by the Available Dataset, or the language you are working with is not supported.  In these cases, you will need to create a dataset from scratch.</p> <p>This page will guide you through the process of annotating a new dataset, from preparing your corpus to generating the annotation files used to train a new model with Propp.</p>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#creating-a-new-dataset-from-scratch","level":2,"title":"Creating a new dataset from scratch.","text":"","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#collecting-texts-for-the-corpus","level":3,"title":"Collecting texts for the corpus","text":"<p>To create a new dataset, you must first build the corpus to be annotated. The corpus should consist of one or more plain <code>.txt</code> files.</p> <p>Text selection should reflect the intended application of the model.  There is a trade-off between corpus size and annotation effort: more data improves coverage but requires more time to annotate.  In practice, a small but diverse corpus is often preferable to a large, homogeneous one (e.g. varying authors, genres, and periods).</p> <p>If the dataset is intended for public release, the source texts must be in the public domain.</p> <p>Copyright duration depends on jurisdiction; see: https://en.wikipedia.org/wiki/List_of_copyright_duration_by_country</p> <p>Suitable sources include:</p> <ul> <li>Wikisource</li> <li>Project Gutenberg</li> <li>Standard Ebooks</li> <li>Gallica</li> </ul>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#example","level":4,"title":"Example","text":"<p>Suppose we want to train a model that detects all mentions referring to <code>animal entities</code>. </p> <p>To construct a minimal corpus, we can start from Wikipedia pages related to animals.</p> <p>Note</p> <p>Wikipedia content is released under the CC BY-SA license and is not in the public domain.  </p> <p>In this example, we construct the corpus from Wikipedia pages corresponding to a small set of animal species (Dog, Guinea pig, Cow, Lion, Jellyfish, Flamingo, Kangaroo).</p> Python Code to Generate the .txt Animals Corpus <pre><code>import wikipedia, os\n\ndef get_wikipedia_text_from_title(url: str) -&gt; str:\n    # Extract the page title from the URL\n    title = url.split(\"/wiki/\")[-1]\n    title = title.replace(\"_\", \" \")\n\n    # Fetch the page content\n    page = wikipedia.page(title, auto_suggest=False)\n    return page.content\n\ncorpus_root_path = \"./animals_corpus\"\nos.makedirs(corpus_root_path, exist_ok=True)\n\nanimals = [\"Dog\", \"Guinea pig\", \"Cow\", \"Lion\", \"Jellyfish\", \"Flamingo\", \"Kangaroo\"]\n\nfor animal in animals:\n    title = animal.replace(\" \", \"_\").lower()\n    filename = title + \".txt\"\n    filepath = os.path.join(corpus_root_path, filename)\n    text = get_wikipedia_text_from_title(animal)\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(text)\n</code></pre> <p>The resulting corpus has the following structure:</p> <pre><code>animals_corpus/\n‚îú‚îÄ‚îÄ cow.txt\n‚îú‚îÄ‚îÄ dog.txt\n‚îú‚îÄ‚îÄ flamingo.txt\n‚îú‚îÄ‚îÄ guinea_pig.txt\n‚îú‚îÄ‚îÄ jellyfish.txt\n‚îú‚îÄ‚îÄ kangaroo.txt\n‚îî‚îÄ‚îÄ lion.txt\n</code></pre> <p>This corpus serves as the foundation for the annotation process. </p> <p>Before starting the actual annotation, it is essential to clearly define what entities should be annotated and how mentions should be identified. </p> <p>This ensures consistency and reproducibility across the dataset.</p>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#annotation-guidelines","level":3,"title":"Annotation Guidelines","text":"<p>Annotation guidelines are structured instructions that define how a dataset should be annotated.</p> <p>They serve as a reference for annotators to ensure that entities are identified consistently and reproducibly across the corpus. </p> <p>Well-defined guidelines are essential for dataset quality, inter-annotator agreement, and for training models that generalize effectively.</p> <p>A good annotation guideline should include:</p> <ol> <li>Precise definitions of the entities or phenomena to annotate.<ul> <li>Clearly distinguish between closely related entity types. For example, in a narrative corpus, ‚ÄúAnimal‚Äù vs. ‚ÄúMythical Creature‚Äù should have unambiguous criteria.</li> </ul> </li> <li>Examples of positive and negative cases.<ul> <li>Positive: ‚ÄúThe <code>lion</code> roared.‚Äù ‚Üí <code>lion</code> is an Animal.</li> <li>Negative: ‚ÄúThe Panthera leo genus is diverse.‚Äù ‚Üí ‚ÄúPanthera leo‚Äù may or may not be annotated depending on your scope.</li> </ul> </li> <li>Annotation rules for edge cases.<ul> <li>Pronouns referring to annotated entities.</li> <li>Proper nouns vs. common nouns.</li> <li>Mentions in foreign or Latin names.</li> </ul> </li> <li>Consistency instructions for dealing with ambiguous or borderline cases.<ul> <li>For example, always prefer the more specific type when a mention could belong to two categories.</li> </ul> </li> </ol> <p>Example (Animal annotation):</p> <p>Annotate all mentions of specific animal species: ‚ÄúDog‚Äù, ‚ÄúLion‚Äù, ‚ÄúJellyfish‚Äù</p> <p>Do not annotate higher-level taxa: ‚ÄúCanine‚Äù, ‚ÄúFelidae‚Äù</p> <p>Annotate pronouns referring to these entities: ‚Äúit‚Äù, ‚Äúthey‚Äù</p> <p>Annotate Latin species names if they appear in context: Panthera leo</p> <p>For more detailed examples and templates of well-structured guidelines, see:</p>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#annotation-guidelines-examples","level":3,"title":"Annotation guidelines examples","text":"","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#litbank","level":4,"title":"Litbank","text":"<p>https://github.com/dbamman/litbank?tab=readme-ov-file</p>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#litbank-fr","level":4,"title":"Litbank-fr","text":"<p>https://github.com/lattice-8094/Proppagate/blob/main/fr/data/Manuel_Annotation_propp.md</p> <p>Transition vers annotation process</p>","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"sacr_dataset_annotation/#annotation-process","level":3,"title":"Annotation Process","text":"<p>Multiple annotation tools exist. For Propp we use SACR</p> Cite this workBibTeX <p>Bruno Oberle. 2018.  SACR: A Drag-and-Drop Based Tool for Coreference Annotation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p> <pre><code>@InProceedings{OBERLE18.178,  \n  author = {Bruno Oberle},\n  title = \"{SACR: A Drag-and-Drop Based Tool for Coreference Annotation}\",\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\n  year = {2018},\n  month = {May 7-12, 2018},\n  address = {Miyazaki, Japan},\n  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and H√©l√®ne Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {979-10-95546-00-9},\n  language = {english}\n  }\n</code></pre> <p>Before starting the annotation in SACR, you need to define the properties configuration : </p> <p>Here we only annotate \"ANIMAL\" entities, but if we wanted to also annotate geopolitical entities (GPE) we would add a new row, one row per entity.</p> <pre><code>PROP:name=EN\n$$$\nANIMAL\nGPE\n</code></pre> <p>Example of other annotation properties: </p> <p>Dataset: long-litbank-fr-PER-only 1 Entity Type(s): ['PER']</p> <p>Dataset: litbank-fr 7 Entity Type(s): ['FAC', 'TIME', 'GPE', 'ORG', 'VEH', 'LOC', 'PER']</p> <p>Dataset: litbank-ru 6 Entity Type(s): ['FAC', 'GPE', 'ORG', 'VEH', 'LOC', 'PER']</p> <p>Dataset: litbank 6 Entity Type(s): ['FAC', 'GPE', 'ORG', 'VEH', 'LOC', 'PER']</p> <p>Dataset: conll2003-NER 4 Entity Type(s): ['PER', 'LOC', 'ORG', 'MISC']</p> <p>Dataset: ontonotes5_english-NER 18 Entity Type(s): ['FAC', 'DATE', 'QUANTITY', 'CARDINAL', 'NORP', 'EVENT', 'PERCENT', 'LANGUAGE', 'ORDINAL', 'GPE', 'ORG', 'LAW', 'TIME', 'LOC', 'WORK_OF_ART', 'PERSON', 'PRODUCT', 'MONEY']</p> <ol> <li>Open the SACR annotation Home Page</li> <li>Load your raw .txt file</li> <li>Paste you Properties configuration</li> <li>Choose the tokenization type: <code>word and punctuation</code></li> <li>Then click the button to <code>parse the data</code></li> <li>Annotate your entities mention spans boundaries </li> <li>Select entity type </li> <li>Link coreferential mentions </li> <li>Name your coreference chain (select a mention and press <code>n</code>)</li> <li>Save your annotated file (press <code>w</code>) Save regularly to avoid losing annotations</li> </ol>    Your browser does not support the video tag.","path":["Datasets","Annotating a new dataset"],"tags":[]},{"location":"step_by_step_processing/","level":1,"title":"Step by Step Processing","text":"","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#1-loading-models","level":2,"title":"1: Loading Models","text":"<pre><code>from propp_fr import load_models\nspacy_model, mentions_detection_model, coreference_resolution_model = load_models()\n</code></pre> <p>Default models are:</p> <ul> <li><code>spacy_model</code>: fr_dep_news_trf</li> <li><code>mentions_detection_model</code>: propp-fr_NER_camembert-large_FAC_GPE_LOC_PER_TIME_VEH</li> <li><code>coreference_resolution_model</code>: propp-fr_coreference-resolution_camembert-large_PER</li> </ul>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#2-loading-a-txt-file","level":2,"title":"2: Loading a .txt File","text":"<pre><code>from propp_fr import load_text_file\ntext_content = load_text_file(\"root_directory/my_french_novel.txt\")\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#3-tokenizing-the-text","level":2,"title":"3: Tokenizing the Text","text":"<p>Break down the text into individual tokens (words and punctuation) with linguistic information:</p> <pre><code>from propp_fr import generate_tokens_df\ntokens_df = generate_tokens_df(text_content, spacy_model)\n</code></pre> <p><code>tokens_df</code> is a <code>pandas.DataFrame</code> where each row represents one token from the text.</p> Column Name Description <code>paragraph_ID</code> Which paragraph the token belongs to <code>sentence_ID</code> Which sentence the token belongs to <code>token_ID_within_sentence</code> Position of the token within its sentence <code>token_ID_within_document</code> Position of the token in the entire document <code>word</code> The actual word as it appears in the text <code>lemma</code> The base/dictionary form of the word <code>byte_onset</code> Starting byte position in the original file <code>byte_offset</code> Ending byte position in the original file <code>POS_tag</code> Part-of-speech tag (noun, verb, adjective, etc.) <code>dependency_relation</code> How the word relates to other words <code>syntactic_head_ID</code> The ID of the word this token depends on","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#4-embedding-tokens","level":2,"title":"4: Embedding Tokens","text":"<p>Transform the tokens into numerical representations (embeddings) that capture their meaning:</p> <pre><code>from propp_fr import load_tokenizer_and_embedding_model, get_embedding_tensor_from_tokens_df\n\n# Load the tokenizer and pre-trained embedding model\ntokenizer, embedding_model = load_tokenizer_and_embedding_model(\n    mentions_detection_model[\"base_model_name\"],\n  )\n\n# Generate embeddings for all tokens\ntokens_embedding_tensor = get_embedding_tensor_from_tokens_df(\n    text_content,\n    tokens_df,\n    tokenizer,\n    embedding_model,\n  )\n</code></pre> <p><code>tokens_embedding_tensor</code> is a <code>torch.tensor</code> object with dimensions <code>[number_of_tokens, embedding_size]</code>.</p> <p>Each row corresponds to one token from <code>tokens_df</code>, preserving the same order.</p> <p>These embeddings will be used as inputs for the <code>mention detection model</code> and the <code>coreference resolution model</code>.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#5-mention-spans-detection","level":2,"title":"5: Mention Spans Detection","text":"<p>Identify all mentions belonging to entities of different types in the text:</p> <ul> <li>Characters (PER): pronouns (je, tu, il, ...), possessive pronouns (mon, ton, son, ...), common nouns (le capitaine, la princesse, ...) and proper nouns (Indiana Delmare, Honor√© de Pardaillan, ...)  </li> <li>Facilities (FAC): chat√™au, sentier, chambre, couloir, ...  </li> <li>Time (TIME): le r√®gne de Louis XIV, ce matin, en juillet, ...  </li> <li>Geo-Political Entities (GPE): Montrouge, France, le petit hameau, ...  </li> <li>Locations (LOC): le sud, Mars, l'oc√©an, le bois, ...  </li> <li>Vehicles (VEH): avion, voitures, cal√®che, v√©los, ...</li> </ul> <pre><code>from propp_fr import generate_entities_df\n\nentities_df = generate_entities_df(\n    tokens_df,\n    tokens_embedding_tensor,\n    mentions_detection_model,\n)\n</code></pre> <p>What this does: Scans through the text to find all mentions of entities.</p> <p>The <code>entities_df</code> object is a <code>pandas.DataFrame</code> where each row represents a detected mention:</p> Column Name Description <code>start_token</code> Token ID where the mention begins <code>end_token</code> Token ID where the mention ends <code>cat</code> Type of the entity <code>confidence</code> Model's confidence score (0-1) for this detection <code>text</code> The actual text of the mention <p>To learn more about how mention detection is performed under the hood, check the Algorithms Section</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#6-adding-linguistic-features","level":2,"title":"6: Adding Linguistic Features","text":"<p>Enrich your entity mentions with additional grammatical and syntactic information:</p> <pre><code>from propp_fr import add_features_to_entities\n\nentities_df = add_features_to_entities(entities_df, tokens_df)\n</code></pre> <p>What this does: Adds detailed linguistic features to each mention, including grammatical properties, syntactic structure, and contextual information.</p> <p>This step adds the following columns to <code>entities_df</code>:</p> Column Name Description <code>mention_len</code> Length of the mention in tokens <code>paragraph_ID</code> Paragraph containing the mention <code>sentence_ID</code> Sentence containing the mention <code>start_token_ID_within_sentence</code> Position where mention starts in its sentence <code>out_to_in_nested_level</code> Nesting depth (outer to inner) <code>in_to_out_nested_level</code> Nesting depth (inner to outer) <code>nested_entities_count</code> Number of entities nested within this mention <code>head_id</code> ID of the syntactic head token <code>head_word</code> The actual head word <code>head_dependency_relation</code> Dependency relation of the head <code>head_syntactic_head_ID</code> ID of the head's syntactic parent <code>POS_tag</code> Part-of-speech tag of the head <code>prop</code> Mention type: pronoun (PRON), common noun (NOM), or proper noun (PROP) <code>number</code> Grammatical number (singular/plural) <code>gender</code> Grammatical gender (masculine/feminine) <code>grammatical_person</code> Grammatical person (1st, 2nd, 3rd) <p>These features are primarily used in the following steps of <code>coreference resolution</code> and <code>character representation</code>, but they can also be leveraged directly for a range of literary and linguistic analyses, such as character centrality, proper name tracking, mention type distribution, gender representation, and narrative perspective.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#7-coreference-resolution","level":2,"title":"7: Coreference Resolution","text":"<p>Link all <code>PER</code> mentions that refer to the same character, creating coreference chains:</p> <pre><code>from propp_fr import perform_coreference\n\nentities_df = perform_coreference(\n    entities_df,\n    tokens_embedding_tensor,\n    coreference_resolution_model,\n    )\n</code></pre> <p>What this does: Groups character mentions into coreference chains where all mentions in a chain refer to the same person. For example, <code>Marie</code>, <code>she</code>, and <code>the young woman</code> might all be linked together as referring to the same character.</p> <p>This step adds one new column to <code>entities_df</code>:</p> Column Name Description <code>COREF</code> ID of the coreference chain this mention belongs to <p>Mentions with the same <code>COREF</code> value refer to the same character. Mentions with different values refer to different characters.</p> <p>To learn more about how coreference resolution is performed under the hood, check the Algorithms Section</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#8-extracting-character-attributes","level":2,"title":"8: Extracting Character Attributes","text":"<p>Identify tokens that describe or relate to characters:</p> <pre><code>from propp_fr import extract_attributes\ntokens_df = extract_attributes(entities_df, tokens_df)\n</code></pre> <p>What this does: Analyzes the syntactic structure around character mentions to identify words that function as attributes, linking them to the characters they describe.</p> <p>This step adds the following columns to <code>tokens_df</code>:</p> Column Name Description <code>is_mention_head</code> Whether this token is the head of a character mention <code>char_att_agent</code> Mention ID if token is an agent attribute, -1 otherwise <code>char_att_patient</code> Mention ID if token is a patient attribute, -1 otherwise <code>char_att_mod</code> Mention ID if token is a modifier attribute, -1 otherwise <code>char_att_poss</code> Mention ID if token is a possessive attribute, -1 otherwise <p>For each token, if it serves as an attribute to a character, the corresponding column contains the syntactic head token ID of the mention. Otherwise, it contains -1.</p> <p>The four attribute types:</p> <ul> <li><code>Agent</code>: verbs where the character is the subject (actions they perform): Marie marche, elle parle</li> <li><code>Patient</code>: verbs where the character is the direct object or passive subject (actions done to them): on pousse Jean, il est suivi</li> <li><code>Modifier</code>: adjectives or nominal predicates describing the character: Hercule est fort, la grande reine, Victor Hugo, l' √©crivain</li> <li><code>Possessive</code>: nouns denoting possessions linked by determiners, de-genitives, or avoir: son √©p√©e, la maison de Alis√©e, il a un chien</li> </ul>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#9-aggregating-character-information","level":2,"title":"9: Aggregating Character Information","text":"<p>Build a unified, structured representation of every character extracted so far:</p> <pre><code>from propp_fr import generate_characters_dict\n\ncharacters_dict = generate_characters_dict(tokens_df, entities_df)\n</code></pre> <p>This function gathers all relevant information from every mention of each character: surface forms, syntactic attributes, and inferred features such as gender or number.</p> <p><code>characters_dict</code> is a list of dictionaries, where each dictionary corresponds to a single character and contains the following fields:</p> Key Description <code>id</code> Character identifier (0 = most frequent) <code>count</code> Total mentions and proportion relative to all character mentions <code>gender</code> Ratio of gendered mentions and inferred overall gender (based on majority evidence) <code>number</code> Ratio of singular/plural mentions and inferred number <code>mentions</code> All surface forms used to refer to the character: proper nouns, common nouns, and pronouns <code>agent</code> List of all agent attributes linked to the character <code>patient</code> list of all patient attributes linked to the character <code>mod</code> list of all modifiers attributes linked to the character <code>poss</code> list of all possessives attributes linked to the character <p>The resulting <code>characters_dict</code> provides a complete, query-ready profile for each character.  This will be useful for narrative analysis, visualization, and computational literary studies.</p>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#saving-generated-output","level":2,"title":"Saving Generated Output","text":"<p>Once processing is complete, you can save the generated files for future use, storing your processed tokens, entities, and character profiles as reusable data so you don‚Äôt need to re-run the entire pipeline.</p> <pre><code>from propp_fr import save_tokens_df, save_entities_df, save_book_file\n\nroot_directory = \"root_directory\"\nfile_name = \"my_french_novel\"\n\nsave_tokens_df(tokens_df, file_name, root_directory)\nsave_entities_df(entities_df, file_name, root_directory)\nsave_book_file(characters_dict, file_name, root_directory)\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"step_by_step_processing/#reloading-processed-files","level":2,"title":"Reloading Processed Files","text":"<p>Later, you can reload the generated files instantly to resume your analysis.</p> <pre><code>from propp_fr import load_text_file, load_tokens_df, load_entities_df, load_book_file\n\nfile_name = \"my_french_novel\"\nroot_directory = \"root_directory\"\n\ntext_content = load_text_file(file_name, root_directory)\ntokens_df = load_tokens_df(file_name, root_directory)\nentities_df = load_entities_df(file_name, root_directory)\ncharacters_dict = load_book_file(file_name, root_directory)\n</code></pre>","path":["User Guide","Basic Processing","Step by Step Processing"],"tags":[]},{"location":"training/","level":1,"title":"Training a new Pipeline","text":"<p>Training a new coreference resolution pipeline from scratch</p>","path":["Training","Training a new Pipeline"],"tags":[]},{"location":"training/#dataset-preparation","level":2,"title":"Dataset Preparation","text":"<p>First, you need an annotated dataset.</p> <p>This dataset should contain:</p> <ul> <li>The raw text files</li> <li>The annotations minimal infos:<ul> <li>The start and end of the annotation (character indexes in the raw text)</li> <li>The label of the annotation (type of entity)</li> <li>The coreference chains ID  </li> </ul> </li> </ul> <p>Ready to use annotated datasets can be downloaded directly from the datasets section.</p>","path":["Training","Training a new Pipeline"],"tags":[]},{"location":"training_coreference_resolution_model/","level":1,"title":"Training a Coreference Resolution Model","text":"","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#load-a-dataset","level":2,"title":"Load a dataset","text":"<p>Select a dataset annotated with Coreference. See Available Dataset.</p> <pre><code>import pandas as pd\n\nimport os, requests, zipfile\nfrom tqdm.auto import tqdm\nfrom collections import Counter\n\n# Where datasets will be stored\nlocal_dataset_directory = \"loaded_datasets\"\nos.makedirs(local_dataset_directory, exist_ok=True)\n\n# Available Datasets: long-litbank-fr-PER-only ; litbank-fr ; litbank ;\n\ndataset_name = \"litbank\" #\n\n# Dataset URL\ndataset_URL_path = (\n    \"https://lattice-8094.github.io/propp/datasets/\"\n    f\"{dataset_name}_propp_minimal_implementation.zip\"\n)\n\n# Local paths\narchive_name = dataset_URL_path.split(\"/\")[-1]\narchive_path = os.path.join(local_dataset_directory, archive_name)\nfiles_directory = os.path.join(local_dataset_directory, archive_name.replace(\".zip\", \"\"))\n\n# Download if needed\nif not os.path.exists(archive_path):\n    print(\"Downloading dataset...\")\n    response = requests.get(dataset_URL_path, stream=True)\n    response.raise_for_status()\n\n    with open(archive_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:\n                f.write(chunk)\n\n    print(f\"Downloaded dataset to {archive_path}\")\nelse:\n    print(\"Archive already exists, skipping download.\")\n\n# Extract if needed\nif not os.path.exists(files_directory):\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n        zip_ref.extractall(files_directory)\n    print(f\"Dataset extracted to {files_directory}\")\nelse:\n    print(\"Dataset already extracted.\")\n</code></pre>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#preprocess-the-dataset","level":2,"title":"Preprocess the dataset","text":"<pre><code>from pathlib import Path\n\nall_files = sorted(list(set([p.stem for p in Path(files_directory).iterdir() if p.is_file()])))\nlen(all_files)\n</code></pre>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#load-the-spacy-model","level":2,"title":"Load the spaCy model","text":"<p>Here, as the dataset is in English, and we have a GPU available, we will use the <code>en_core_web_trf</code> model.</p> <pre><code>from propp_fr import load_spacy_model\n\nspacy_model_name = \"en_core_web_trf\"\n# spacy_model_name = \"fr_dep_news_trf\"\nspacy_model = load_spacy_model(spacy_model_name)\n</code></pre> <p>Define Preprocessing Functions</p> <pre><code>def realign_tokens_offsets(tokens_df, entities_df):\n    start_tokens = []\n    end_tokens = []\n    new_byte_onsets = []\n    new_byte_offsets = []\n\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        start_token = tokens_df[tokens_df[\"byte_offset\"] &gt; mention_byte_onset].index.min()\n        end_token = tokens_df[tokens_df[\"byte_onset\"] &lt; mention_byte_offset].index.max()\n        new_byte_onsets.append(tokens_df.loc[start_token, \"byte_onset\"])\n        new_byte_offsets.append(tokens_df.loc[end_token, \"byte_offset\"])\n\n        start_tokens.append(start_token)\n        end_tokens.append(end_token)\n\n    entities_df[\"start_token\"] = start_tokens\n    entities_df[\"end_token\"] = end_tokens\n    entities_df[\"byte_onset\"] = new_byte_onsets\n    entities_df[\"byte_offset\"] = new_byte_offsets\n\n    return entities_df\n\ndef extract_mention_text(text_content, entities_df):\n    mention_texts = []\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        mention_texts.append(text_content[mention_byte_onset:mention_byte_offset])\n    entities_df[\"text\"] = mention_texts\n    entities_df[\"text\"] = entities_df[\"text\"].astype(str)\n    return entities_df\n</code></pre> <p>Preprocess the dataset to make it ready for mention spans detection training.</p> <pre><code>from propp_fr import load_text_file, generate_tokens_df, save_tokens_df\nfrom propp_fr import load_tokens_df, load_entities_df, add_features_to_entities, save_entities_df\n\nfor file_name in tqdm(all_files):\n    if os.path.exists(os.path.join(files_directory, file_name + \".tokens\")):\n        continue\n        # pass\n    text_content = load_text_file(file_name, files_directory)\n    tokens_df = generate_tokens_df(text_content, spacy_model, verbose=0)\n    entities_df = load_entities_df(file_name, files_directory)\n\n    entities_df = realign_tokens_offsets(tokens_df, entities_df)\n    entities_df = extract_mention_text(text_content, entities_df)\n\n    entities_df = add_features_to_entities(entities_df, tokens_df)\n\n    if spacy_model_name == \"en_core_web_trf\":\n        entities_df[\"gender\"] = \"Not_Assigned\" # Not available for english\n        entities_df[\"number\"] = \"Not_Assigned\" # Not available for english\n        entities_df[\"grammatical_person\"] = \"4\" # Not available for english\n\n    save_entities_df(entities_df, file_name, files_directory)\n    save_tokens_df(tokens_df, file_name, files_directory)\n</code></pre> <pre><code>from propp_fr import coreference_resolution_LOOCV_full_model_training, generate_coref_model_card_from_LOOCV_directory\n\nhelp(coreference_resolution_LOOCV_full_model_training)\n\nfrom collections import Counter\n\nNER_categories = []\nfor file_name in all_files:\n    entities_df = load_entities_df(file_name, files_directory)\n    NER_categories.extend(entities_df[\"cat\"].tolist())\nprint(Counter(NER_categories))\nNER_cat_list = list(set(NER_categories))\nprint(NER_cat_list)\n</code></pre>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#define-training-parameters","level":2,"title":"Define Training Parameters","text":"<pre><code>subword_pooling_strategy = \"first_last\"\nnested_levels = [0]\ntagging_scheme = \"BIOES\"\ntest_split = [file for file in all_files if file.startswith(\"test\")]\nprint(f\"Test split: {len(test_split):,}\")\n\nmodel_name = \"FacebookAI/roberta-large\"\nembedding_model_name = model_name.split(\"/\")[-1]\ntrained_model_directory = os.path.join(files_directory, f\"mentions_detection_model_{embedding_model_name}\")\n</code></pre>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#train-and-evaluate-the-model-and-generate-the-model-card","level":2,"title":"Train and Evaluate the model And Generate the Model Card","text":"<pre><code>mentions_detection_LOOCV_full_model_training(files_directory=files_directory,\n                                             trained_model_directory=trained_model_directory,\n                                             model_name=model_name,\n                                             subword_pooling_strategy=subword_pooling_strategy,\n                                             nested_levels=nested_levels,\n                                             NER_cat_list=NER_cat_list,\n                                             tagging_scheme=tagging_scheme,\n                                             train_final_model=False,\n                                             files_to_use_in_cross_validation=[test_split],\n                                             verbose=0)\n\ngenerate_NER_model_card_from_LOOCV_directory(trained_model_directory)\n</code></pre> Model Card Example <p>language: fr tags: - NER   - literary-texts   - nested-entities   - propp-fr   license: apache-2.0   metrics:   - f1   - precision   - recall   base_model:   - FacebookAI/roberta-large   pipeline_tag: token-classification</p>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#introduction","level":2,"title":"INTRODUCTION:","text":"<p>This model, developed as part of the propp-fr project, is a NER model built on top of roberta-large embeddings, trained to predict nested entities in french, specifically for literary texts.</p> <p>The predicted entities are:</p> <ul> <li>PER: Person names (real or fictional)<ul> <li>ORG: Organizations (companies, institutions)</li> <li>LOC: Geographical locations (non-political: mountains, rivers, cities)</li> <li>MISC: Miscellaneous entities (events, nationalities, products, etc.)</li> </ul> </li> </ul>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#model-performances-test-set","level":2,"title":"MODEL PERFORMANCES (TEST SET):","text":"NER_tag precision recall f1_score support support % LOC 93.74% 93.41% 93.57% 1,668 29.53% ORG 92.98% 93.26% 93.12% 1,661 29.41% PER 97.90% 98.02% 97.96% 1,617 28.63% MISC 81.33% 81.91% 81.62% 702 12.43% micro_avg 93.16% 93.25% 93.21% 5,648 100.00% macro_avg 91.49% 91.65% 91.57% 5,648 100.00%","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#training-parameters","level":2,"title":"TRAINING PARAMETERS:","text":"<ul> <li>Entities types: ['MISC', 'ORG', 'PER', 'LOC']</li> <li>Tagging scheme: BIOES</li> <li>Nested entities levels: [0]</li> <li>Split strategy: Leave-one-out cross-validation (1393 files)</li> <li>Train/Validation split: 0.85 / 0.15</li> <li>Batch size: 16</li> <li>Initial learning rate: 0.00014</li> </ul>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#model-architecture","level":2,"title":"MODEL ARCHITECTURE:","text":"<p>Model Input: Maximum context roberta-large embeddings (1024 dimensions)</p> <ul> <li> <p>Locked Dropout: 0.5</p> </li> <li> <p>Projection layer:</p> <ul> <li>layer type: highway layer</li> <li>input: 1024 dimensions</li> <li>output: 2048 dimensions</li> </ul> </li> <li> <p>BiLSTM layer:</p> <ul> <li>input: 2048 dimensions</li> <li>output: 256 dimensions (hidden state)</li> </ul> </li> <li> <p>Linear layer:</p> <ul> <li>input: 256 dimensions</li> <li>output: 17 dimensions (predicted labels with BIOES tagging scheme)</li> </ul> </li> <li> <p>CRF layer</p> </li> </ul> <p>Model Output: BIOES labels sequence</p>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#how-to-use","level":2,"title":"HOW TO USE:","text":"<p>Propp Documentation</p>","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_coreference_resolution_model/#predictions-confusion-matrix","level":2,"title":"PREDICTIONS CONFUSION MATRIX:","text":"Gold Labels LOC ORG PER MISC O support LOC 1,558 38 1 22 49 1,668 ORG 30 1,549 2 12 68 1,661 PER 6 7 1,585 2 17 1,617 MISC 25 24 10 575 68 702 O 43 48 21 96 0 208","path":["Training","Training a Model","Training a Coreference Resolution Model"],"tags":[]},{"location":"training_mention_detection_model/","level":1,"title":"Training a Mention Detection Model","text":"","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#load-a-dataset","level":2,"title":"Load a dataset","text":"<p>Copy / Paste this code to define the dataset loading functions</p> Dataset Loading Functions <pre><code>import pandas as pd\n\nimport os, requests, zipfile\nfrom tqdm.auto import tqdm\nfrom collections import Counter\n\nfrom pathlib import Path\nimport json\nfrom collections import Counter\n\nfrom propp_fr import load_spacy_model\nfrom propp_fr import load_text_file, generate_tokens_df, save_tokens_df\nfrom propp_fr import load_tokens_df, load_entities_df, add_features_to_entities, save_entities_df\nfrom propp_fr import mentions_detection_LOOCV_full_model_training, generate_NER_model_card_from_LOOCV_directory\n\ndef realign_tokens_offsets(tokens_df, entities_df):\n    start_tokens = []\n    end_tokens = []\n    new_byte_onsets = []\n    new_byte_offsets = []\n\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        start_token = tokens_df[tokens_df[\"byte_offset\"] &gt; mention_byte_onset].index.min()\n        end_token = tokens_df[tokens_df[\"byte_onset\"] &lt; mention_byte_offset].index.max()\n        new_byte_onsets.append(tokens_df.loc[start_token, \"byte_onset\"])\n        new_byte_offsets.append(tokens_df.loc[end_token, \"byte_offset\"])\n\n        start_tokens.append(start_token)\n        end_tokens.append(end_token)\n\n    entities_df[\"start_token\"] = start_tokens\n    entities_df[\"end_token\"] = end_tokens\n    entities_df[\"byte_onset\"] = new_byte_onsets\n    entities_df[\"byte_offset\"] = new_byte_offsets\n\n    return entities_df\n\ndef extract_mention_text(text_content, entities_df):\n    mention_texts = []\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        mention_texts.append(text_content[mention_byte_onset:mention_byte_offset])\n    entities_df[\"text\"] = mention_texts\n    entities_df[\"text\"] = entities_df[\"text\"].astype(str)\n    return entities_df\n\ndef load_dataset(dataset_name, local_dataset_directory=\"loaded_datasets\", preprocess=False, force_download=False):\n    # Where datasets will be stored\n    os.makedirs(local_dataset_directory, exist_ok=True)\n\n    # Dataset URL\n    dataset_URL_path = (\n        \"https://lattice-8094.github.io/propp/datasets/\"\n        f\"{dataset_name}_propp_minimal_implementation.zip\"\n    )\n\n    # Local paths\n    archive_name = dataset_URL_path.split(\"/\")[-1]\n    archive_path = os.path.join(local_dataset_directory, archive_name)\n    files_directory = os.path.join(local_dataset_directory, dataset_name.replace(\".zip\", \"\"))\n\n    # Download if needed\n    if not os.path.exists(archive_path) or force_download:\n        print(\"Downloading dataset...\")\n        response = requests.get(dataset_URL_path, stream=True)\n        response.raise_for_status()\n\n        with open(archive_path, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n\n        print(f\"Downloaded dataset to {archive_path}\")\n    else:\n        print(\"Archive already exists, skipping download.\")\n\n    # Extract if needed\n    if not os.path.exists(files_directory) or force_download:\n        print(\"Extracting dataset...\")\n        with zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n            zip_ref.extractall(files_directory)\n        print(f\"Dataset extracted to {files_directory}\")\n    else:\n        print(\"Dataset already extracted.\")\n\n    if os.path.exists(os.path.join(files_directory, \"split_config.json\")):\n        with open(os.path.join(files_directory, \"split_config.json\"), \"r\") as f:\n            config = json.load(f)\n            if \"dataset_language\" in config.keys():\n                dataset_language = config[\"dataset_language\"]\n                print(dataset_language)\n            else:\n                print(f\"No language configuration. Default to English.\")\n                dataset_language = \"en\"\n    else:\n        print(f\"Config not found for '{dataset_name}'\")\n        dataset_language = \"en\"\n\n    if dataset_language == \"fr\":\n        spacy_model_name = \"fr_dep_news_trf\"\n    elif dataset_language == \"ru\":\n        spacy_model_name = \"ru_core_news_lg\"\n    else:\n        spacy_model_name = \"en_core_web_trf\"\n\n    all_entities_files = sorted(list(set([p.stem for p in Path(files_directory).iterdir() if p.suffix == \".entities\"])))\n    all_tokens_files = sorted(list(set([p.stem for p in Path(files_directory).iterdir() if p.suffix == \".tokens\"])))\n\n    if len(all_entities_files) != len(all_tokens_files) or preprocess:\n        spacy_model = load_spacy_model(spacy_model_name)\n\n    for file_name in tqdm(all_entities_files, leave=False):\n        if os.path.exists(os.path.join(files_directory, file_name + \".tokens\")) and not preprocess:\n            continue\n        text_content = load_text_file(file_name, files_directory)\n        tokens_df = generate_tokens_df(text_content, spacy_model, verbose=0)\n        entities_df = load_entities_df(file_name, files_directory)\n\n        entities_df = realign_tokens_offsets(tokens_df, entities_df)\n        entities_df = extract_mention_text(text_content, entities_df)\n\n        entities_df = add_features_to_entities(entities_df, tokens_df)\n\n        if dataset_language != \"fr\":\n            entities_df[\"gender\"] = \"Not_Assigned\" # Not available for english\n            entities_df[\"number\"] = \"Not_Assigned\" # Not available for english\n            entities_df[\"grammatical_person\"] = \"4\" # Not available for english\n\n        save_entities_df(entities_df, file_name, files_directory)\n        save_tokens_df(tokens_df, file_name, files_directory)\n\n    print(f\"Dataset Directory: {files_directory}\")\n</code></pre> <p>Choose the Dataset to load. See Available Dataset.</p> <pre><code># Available Datasets: [\"long-litbank-fr-PER-only\", \"litbank-fr\", \"litbank-ru\", \"litbank\", \"conll2003-NER\", \"ontonotes5_english-NER\"]\ndataset_name = \"litbank\"\n\nlocal_dataset_directory=\"loaded_datasets\"\nload_dataset(dataset_name, local_dataset_directory=local_dataset_directory)\n\nfiles_directory = os.path.join(local_dataset_directory, dataset_name)\n\nif os.path.exists(os.path.join(files_directory, \"split_config.json\")):\n    with open(os.path.join(files_directory, \"split_config.json\"), \"r\") as f:\n        config = json.load(f)\n    print(f\"\\nConfiguration:\\n{list(config.keys())}\")\n</code></pre> <pre><code>Archive already exists, skipping download.  \nDataset already extracted.  \n['en']  \n\nDataset Directory: loaded_datasets/litbank  \n</code></pre> <p>Dataset contains a <code>config.json</code> file with the language of the dataset, and possibly the <code>train</code>, <code>valid</code>, <code>test</code> splits</p> <pre><code>Configuration:  \n['test_0', 'test_1', 'test_2', 'test_3', 'test_4', 'test_5', 'test_6', 'test_7', 'test_8', 'test_9', 'dataset_language']\n</code></pre> <pre><code>import os, requests, zipfile\nfrom tqdm.auto import tqdm\nfrom collections import Counter\n\n# Where datasets will be stored\nlocal_dataset_directory = \"loaded_datasets\"\nos.makedirs(local_dataset_directory, exist_ok=True)\n\n# Available Datasets: long-litbank-fr-PER-only ; litbank-fr ; litbank ; conll2003-NER\n\ndataset_name = \"conll2003-NER\" #\n\n# Dataset URL\ndataset_URL_path = (\n    \"https://lattice-8094.github.io/propp/datasets/\"\n    f\"{dataset_name}_propp_minimal_implementation.zip\"\n)\n\n# Local paths\narchive_name = dataset_URL_path.split(\"/\")[-1]\narchive_path = os.path.join(local_dataset_directory, archive_name)\nfiles_directory = os.path.join(local_dataset_directory, archive_name.replace(\".zip\", \"\"))\n\n# Download if needed\nif not os.path.exists(archive_path):\n    print(\"Downloading dataset...\")\n    response = requests.get(dataset_URL_path, stream=True)\n    response.raise_for_status()\n\n    with open(archive_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:\n                f.write(chunk)\n\n    print(f\"Downloaded dataset to {archive_path}\")\nelse:\n    print(\"Archive already exists, skipping download.\")\n\n# Extract if needed\nif not os.path.exists(files_directory):\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n        zip_ref.extractall(files_directory)\n    print(f\"Dataset extracted to {files_directory}\")\nelse:\n    print(\"Dataset already extracted.\")\n</code></pre>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#preprocess-the-dataset","level":2,"title":"Preprocess the dataset","text":"<pre><code>from pathlib import Path\n\nall_files = sorted(list(set([p.stem for p in Path(files_directory).iterdir() if p.is_file()])))\nlen(all_files)\n</code></pre>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#load-the-spacy-model","level":2,"title":"Load the spaCy model","text":"<p>Here, as the dataset is in English, and we have a GPU available, we will use the <code>en_core_web_trf</code> model.</p> <pre><code>from propp_fr import load_spacy_model\n\nspacy_model = load_spacy_model(\"en_core_web_trf\")\n</code></pre> <p>Define Preprocessing Functions</p> <pre><code>def realign_tokens_offsets(tokens_df, entities_df):\n    start_tokens = []\n    end_tokens = []\n    new_byte_onsets = []\n    new_byte_offsets = []\n\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        start_token = tokens_df[tokens_df[\"byte_offset\"] &gt; mention_byte_onset].index.min()\n        end_token = tokens_df[tokens_df[\"byte_onset\"] &lt; mention_byte_offset].index.max()\n        new_byte_onsets.append(tokens_df.loc[start_token, \"byte_onset\"])\n        new_byte_offsets.append(tokens_df.loc[end_token, \"byte_offset\"])\n\n        start_tokens.append(start_token)\n        end_tokens.append(end_token)\n\n    entities_df[\"start_token\"] = start_tokens\n    entities_df[\"end_token\"] = end_tokens\n    entities_df[\"byte_onset\"] = new_byte_onsets\n    entities_df[\"byte_offset\"] = new_byte_offsets\n\n    return entities_df\n\ndef extract_mention_text(text_content, entities_df):\n    mention_texts = []\n    for mention_byte_onset, mention_byte_offset in entities_df[[\"byte_onset\", \"byte_offset\"]].values:\n        mention_texts.append(text_content[mention_byte_onset:mention_byte_offset])\n    entities_df[\"text\"] = mention_texts\n    entities_df[\"text\"] = entities_df[\"text\"].astype(str)\n    return entities_df\n</code></pre> <p>Preprocess the dataset to make it ready for mention spans detection training.</p> <pre><code>from propp_fr import load_text_file, generate_tokens_df, save_tokens_df\nfrom propp_fr import load_tokens_df, load_entities_df, add_features_to_entities, save_entities_df\n\nfor file_name in tqdm(all_files):\n    if os.path.exists(os.path.join(files_directory, file_name + \".tokens\")):\n        continue\n    text_content = load_text_file(file_name, files_directory)\n    tokens_df = generate_tokens_df(text_content, spacy_model, verbose=0)\n    entities_df = load_entities_df(file_name, files_directory)\n\n    entities_df = realign_tokens_offsets(tokens_df, entities_df)\n    entities_df = extract_mention_text(text_content, entities_df)\n\n    entities_df = add_features_to_entities(entities_df, tokens_df)\n    entities_df[\"gender\"] = \"Not_Assigned\" # Not available for english\n    entities_df[\"number\"] = \"Not_Assigned\" # Not available for english\n    entities_df[\"grammatical_person\"] = \"4\" # Not available for english\n\n    save_entities_df(entities_df, file_name, files_directory)\n    save_tokens_df(tokens_df, file_name, files_directory)\n</code></pre> <pre><code>from propp_fr import mentions_detection_LOOCV_full_model_training, generate_NER_model_card_from_LOOCV_directory\n\nfrom collections import Counter\n\nNER_categories = []\nfor file_name in all_files:\n    entities_df = load_entities_df(file_name, files_directory)\n    NER_categories.extend(entities_df[\"cat\"].tolist())\nprint(Counter(NER_categories))\nNER_cat_list = list(set(NER_categories))\nprint(NER_cat_list)\n</code></pre>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#define-training-parameters","level":2,"title":"Define Training Parameters","text":"<pre><code>subword_pooling_strategy = \"first_last\"\nnested_levels = [0]\ntagging_scheme = \"BIOES\"\ntest_split = [file for file in all_files if file.startswith(\"test\")]\nprint(f\"Test split: {len(test_split):,}\")\n\nmodel_name = \"FacebookAI/roberta-large\"\nembedding_model_name = model_name.split(\"/\")[-1]\ntrained_model_directory = os.path.join(files_directory, f\"mentions_detection_model_{embedding_model_name}\")\n</code></pre>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#train-and-evaluate-the-model-and-generate-the-model-card","level":2,"title":"Train and Evaluate the model And Generate the Model Card","text":"<pre><code>mentions_detection_LOOCV_full_model_training(files_directory=files_directory,\n                                             trained_model_directory=trained_model_directory,\n                                             model_name=model_name,\n                                             subword_pooling_strategy=subword_pooling_strategy,\n                                             nested_levels=nested_levels,\n                                             NER_cat_list=NER_cat_list,\n                                             tagging_scheme=tagging_scheme,\n                                             train_final_model=False,\n                                             files_to_use_in_cross_validation=[test_split],\n                                             verbose=0)\n\ngenerate_NER_model_card_from_LOOCV_directory(trained_model_directory)\n</code></pre> Model Card Example <p>language: fr tags: - NER   - literary-texts   - nested-entities   - propp-fr   license: apache-2.0   metrics:   - f1   - precision   - recall   base_model:   - FacebookAI/roberta-large   pipeline_tag: token-classification</p>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#introduction","level":2,"title":"INTRODUCTION:","text":"<p>This model, developed as part of the propp-fr project, is a NER model built on top of roberta-large embeddings, trained to predict nested entities in french, specifically for literary texts.</p> <p>The predicted entities are:</p> <ul> <li>PER: Person names (real or fictional)<ul> <li>ORG: Organizations (companies, institutions)</li> <li>LOC: Geographical locations (non-political: mountains, rivers, cities)</li> <li>MISC: Miscellaneous entities (events, nationalities, products, etc.)</li> </ul> </li> </ul>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#model-performances-test-set","level":2,"title":"MODEL PERFORMANCES (TEST SET):","text":"NER_tag precision recall f1_score support support % LOC 93.74% 93.41% 93.57% 1,668 29.53% ORG 92.98% 93.26% 93.12% 1,661 29.41% PER 97.90% 98.02% 97.96% 1,617 28.63% MISC 81.33% 81.91% 81.62% 702 12.43% micro_avg 93.16% 93.25% 93.21% 5,648 100.00% macro_avg 91.49% 91.65% 91.57% 5,648 100.00%","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#training-parameters","level":2,"title":"TRAINING PARAMETERS:","text":"<ul> <li>Entities types: ['MISC', 'ORG', 'PER', 'LOC']</li> <li>Tagging scheme: BIOES</li> <li>Nested entities levels: [0]</li> <li>Split strategy: Leave-one-out cross-validation (1393 files)</li> <li>Train/Validation split: 0.85 / 0.15</li> <li>Batch size: 16</li> <li>Initial learning rate: 0.00014</li> </ul>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#model-architecture","level":2,"title":"MODEL ARCHITECTURE:","text":"<p>Model Input: Maximum context roberta-large embeddings (1024 dimensions)</p> <ul> <li> <p>Locked Dropout: 0.5</p> </li> <li> <p>Projection layer:</p> <ul> <li>layer type: highway layer</li> <li>input: 1024 dimensions</li> <li>output: 2048 dimensions</li> </ul> </li> <li> <p>BiLSTM layer:</p> <ul> <li>input: 2048 dimensions</li> <li>output: 256 dimensions (hidden state)</li> </ul> </li> <li> <p>Linear layer:</p> <ul> <li>input: 256 dimensions</li> <li>output: 17 dimensions (predicted labels with BIOES tagging scheme)</li> </ul> </li> <li> <p>CRF layer</p> </li> </ul> <p>Model Output: BIOES labels sequence</p>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#how-to-use","level":2,"title":"HOW TO USE:","text":"<p>Propp Documentation</p>","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"training_mention_detection_model/#predictions-confusion-matrix","level":2,"title":"PREDICTIONS CONFUSION MATRIX:","text":"Gold Labels LOC ORG PER MISC O support LOC 1,558 38 1 22 49 1,668 ORG 30 1,549 2 12 68 1,661 PER 6 7 1,585 2 17 1,617 MISC 25 24 10 575 68 702 O 43 48 21 96 0 208","path":["Training","Training a Model","Training a Mention Detection Model"],"tags":[]},{"location":"use_cases/","level":1,"title":"Use Cases","text":"","path":["Use Cases"],"tags":[]},{"location":"use_cases/#detective-archetype","level":2,"title":"Detective Archetype","text":"<p>Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature</p> <p>This research explores the evolution of the detective archetype in French detective fiction through computational analysis. Using quantitative methods and character-level embeddings, we show that a supervised model is able to capture the unity of the detective archetype across 150 years of literature, from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding, the study demonstrates how the detective figure evolves from a secondary narrative role to become the central character and the ‚Äúreasoning machine‚Äù of the classical detective story. In the aftermath of the Second World War, with the importation of the hardboiled tradition into France, the archetype becomes more complex, navigating the genre‚Äôs turn toward social violence and moral ambiguity.</p> <p> </p> Cite this workBibTeX <p>Jean Barr√©, Olga Seminck, Antoine Bourgois, and Thierry Poibeau.  2025.  Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature.  In Anthology of Computers and the Humanities, 3:983‚Äì999.</p> <pre><code>@article{barre_detectives_2025,\n  title = {Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature},\n  author = {Jean Barr√© and Olga Seminck and Antoine Bourgois and Thierry Poibeau},\n  year = {2025},\n  address = \"Luxembourg\",\n  journal = {Anthology of Computers and the Humanities},\n  volume = {3},\n  pages = {983--999},\n  editor = {Taylor Arnold, Margherita Fantoli, and Ruben Ros},\n  doi = {10.63744/SMbYIWcHZj87},\n  url = {https://anthology.ach.org/volumes/vol0003/modeling-construction-of-literary-archetype-case/},\n}\n</code></pre>","path":["Use Cases"],"tags":[]},{"location":"use_cases/#character-representation-ontology","level":2,"title":"Character Representation - Ontology","text":"","path":["Use Cases"],"tags":[]}]}